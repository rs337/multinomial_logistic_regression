{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ranSpOffE3VY"
      },
      "source": [
        "# CA6011 Deep Learning for NLP: Week 1 Lab - Logistic Regression\n",
        "\n",
        "**Logistic Regression** (sometimes called logit regression) estimates the parameters of a logistic model. In **binary logistic regression** there is a single dependent variable; in **multinomial logistic regression**, there are multiple dependent variables.\n",
        "\n",
        "In this lab you will code, more or less from scratch, first (standard) logistic regression, and then multinomial logistic regression.\n",
        "\n",
        "We provide a coding framework with a lot of the code in place, for you to add to. At two points during the lab, we will share partial solutions: after 45mins and after 90mins.\n",
        "\n",
        "The remainder of the lab (for which we don't provide solutions) is for you to complete on your own, and must be submitted for assessment for Week 1 of this module (see module handbook).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px72Z8CKj3nO"
      },
      "source": [
        "Troughout this notebook, additional information can be found in the Week 1 lecture slides and Chapter 4 of the book **[Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/4.pdf)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A8M-G8P2Ej5A"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline allows for displaying plots directly in the Jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# NumPy is a library for numerical computations, with support for arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# tqdm is used for creating progress bars to track the progress of for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Matplotlib is a plotting library, and we use it for tracking the loss values across iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# make_classification is a function in the scikit-learn library to generate a random classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# StandardScaler is a function in scikit-learn for standardising (scaling) the features of a dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# train_test_split is a function in scikit-learn for splitting a dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# accuracy_score is a function in scikit-learn that implements the accuracy metric to evaluate classification models\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# IPython.display allows clearing the output in a Jupyter notebook cell\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Seed for reproducibility, fixing the random seed ensures consistent results when rerunning the code\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcvaqGkhFWae"
      },
      "source": [
        "In this section, we develop an implementation of logistic regression and apply it to a synthetic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOx-iPViHJTA"
      },
      "source": [
        "## 2  Multinomial Logistic Regression\n",
        "\n",
        "Standard logistic regression is for binary classification (there is one class, and instances either belong to it or they don't). **Multinomial Logistic Regression** is a generalisation to multi-class classification. Instead of using the Sigmoid function, it uses Softmax to obtain a probability distribution over the $K$ classes.\n",
        "\n",
        "Other than that, we also now need $K$ weight vectors and gradient vectors rather than single ones as in standard (binary) logistic regression.\n",
        "\n",
        "\n",
        "**Multinomial Logistic Regression modelling:**\n",
        "\n",
        "$P(Y_k=1 | X) = \\frac{\\text{exp}(W_k \\cdot X + b_k)}{\\sum_{j=1}^{K} \\text{exp}(W_j \\cdot X + b_j)}$\n",
        "\n",
        " - $K$ is the number of classes,\n",
        " - $\\frac{\\text{exp}(W_k \\cdot X + b_k)}{\\sum_{j=1}^{K} \\text{exp}(W_j \\cdot X + b_j)}$ is the softmax function\n",
        "\n",
        "\n",
        " Refer to the lecture slides for details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzq_MUvFHQ97"
      },
      "source": [
        "In this section, we develop an implementation of multinomial logistic regression, and apply it to a real-world dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xke41_TIqf8"
      },
      "source": [
        "#### 2.1 Dataset\n",
        "\n",
        "Instead of creating a toy dataset as we did in Section 1.1, in this section we'll use a real world dataset.\n",
        "\n",
        "We use the **[AGnews dataset](https://huggingface.co/datasets/wangrongsheng/ag_news)** composed of news articles categorised by domain. The dataset contains 4 domains: *World*; *Sports*, *Business*, and *Science/Technology*.\n",
        "\n",
        "The dataset is already divided into train and test sets containing 120K and 7.6K samples, respectively.\n",
        "\n",
        "For this exercise, we are going to exclude the World category.\n",
        "\n",
        "First, we download all the files composing the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tS5Y0MuueGuX"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvD2TWlRxZV6"
      },
      "source": [
        "Now we are able to load the dataset and preprocess it. We use the labels *sports, business* and *science* for our three news domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "3NyGV3Nhev-p",
        "outputId": "ef66f27a-f02f-405e-e659-c83a69704e7f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      1  Wall St. Bears Claw Back Into the Black (Reute...\n",
              "1      1  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
              "2      1  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
              "3      1  Iraq Halts Oil Exports from Main Southern Pipe...\n",
              "4      1  Oil prices soar to all-time record, posing new...\n",
              "5      1  Stocks End Up, But Near Year Lows (Reuters) Re...\n",
              "6      1  Money Funds Fell in Latest Week (AP) AP - Asse...\n",
              "7      1  Fed minutes show dissent over inflation (USATO...\n",
              "8      1  Safety Net (Forbes.com) Forbes.com - After ear...\n",
              "9      1  Wall St. Bears Claw Back Into the Black  NEW Y..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# labels contained in the dataset\n",
        "labels = ['sports', 'business', 'science']\n",
        "\n",
        "# dataset is saved in a CSV file with no header and each column in separated by comma\n",
        "# the file has the following structure:\n",
        "# gold_label , title , body\n",
        "df = pd.read_csv('agnews_train.csv', header=None)\n",
        "\n",
        "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
        "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
        "df = df[df[0] != 1]\n",
        "\n",
        "# we create a 'label' column, subtract 2 from each gold label so we obtain a direct mapping with our list of labels:\n",
        "# 0 -> sports ; 1 -> business ; 2 -> science\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "# we concatenate title and body to obtain a unified text\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "# remove the original 3 columns to obtain our final processed dataset, containing 2 columns: label and text\n",
        "# the original 3 columns are the unmatched labels, the titles and the bodies\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3fOaD825c8w"
      },
      "source": [
        "In order to process text, we need to identify features that our model can use to classify texts.\n",
        "\n",
        "We use four hand-crafted features:\n",
        "\n",
        "1.  natural logarithm of text length;\n",
        "2.  number of sports words in the text;\n",
        "3.  number of business words in the text;\n",
        "4.  number of science words in the text.\n",
        "\n",
        "Next, we download a list of words for each target label to compute our features. We get the lists of words from [Enchanted Learning - Wordlist](https://www.enchantedlearning.com/wordlist/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tAghO6DnxUdt"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XWzN3nBPcWp50f_DjC2rpS7G2PvzjvmB' -O business.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1u3KQGgkFTN8s4fTGzJJOsmR4MGVlusp1' -O science.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1g_gYaij_xn1HwGebruK3JJRZy30P7E49' -O sports.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2rzItBYIvLrT"
      },
      "outputs": [],
      "source": [
        "# computes the features of the given text and returns a list of features in the format:\n",
        "# [ln text_len, # sports words, # business words, # science words]\n",
        "def get_features_text(text: str):\n",
        "  \"\"\"\n",
        "    Calculated text features for the given text.\n",
        "\n",
        "    Args:\n",
        "        text (string): string containing the text.\n",
        "\n",
        "    Returns:\n",
        "        List with dimension 4: list containing the features extracted from the text in the order\n",
        "                                [ln text_len, # sports words, # business words, # science words].\n",
        "    \"\"\"\n",
        "  import math\n",
        "  # divide the text into words\n",
        "  words = text.split()\n",
        "\n",
        "  # initialise the features list adding the ln of the text length\n",
        "  features = [math.log(len(words))]\n",
        "\n",
        "  # for each target label we have in the dataset\n",
        "  for label in labels:\n",
        "    # open the list of words of the current label\n",
        "    with open(f'{label}.txt', 'r') as label_file:\n",
        "      # get all the words associated to the current label\n",
        "      # the file contains 1 word for each line, so we split the whole text with \\n\n",
        "      label_words = label_file.read().split('\\n')\n",
        "    label_count = 0\n",
        "    # for each word in the text,\n",
        "    for word in words:\n",
        "      # check if the word is present in the list of words associated to the current label\n",
        "      if word in label_words:\n",
        "        # if it's present add 1 to the current count\n",
        "        label_count += 1\n",
        "    # append the final count to the features list\n",
        "    features.append(label_count)\n",
        "  # return the features list [ln text_len, # sports words, # business words, # science words]\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0UqpNoEFxZ-E",
        "outputId": "b06d2529-c3a8-455a-cbe7-845c1a0f7d66"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
              "      <td>[3.044522437723423, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>[3.58351893845611, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
              "      <td>[3.58351893845611, 0, 3, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>[3.58351893845611, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>[3.6109179126442243, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
              "      <td>[3.6375861597263857, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
              "      <td>[3.5553480614894135, 0, 3, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
              "      <td>[3.6635616461296463, 0, 4, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
              "      <td>[4.219507705176107, 1, 8, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
              "      <td>[3.1354942159291497, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text  \\\n",
              "0      1  Wall St. Bears Claw Back Into the Black (Reute...   \n",
              "1      1  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
              "2      1  Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
              "3      1  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
              "4      1  Oil prices soar to all-time record, posing new...   \n",
              "5      1  Stocks End Up, But Near Year Lows (Reuters) Re...   \n",
              "6      1  Money Funds Fell in Latest Week (AP) AP - Asse...   \n",
              "7      1  Fed minutes show dissent over inflation (USATO...   \n",
              "8      1  Safety Net (Forbes.com) Forbes.com - After ear...   \n",
              "9      1  Wall St. Bears Claw Back Into the Black  NEW Y...   \n",
              "\n",
              "                   text_features  \n",
              "0   [3.044522437723423, 0, 0, 0]  \n",
              "1    [3.58351893845611, 1, 1, 0]  \n",
              "2    [3.58351893845611, 0, 3, 0]  \n",
              "3    [3.58351893845611, 0, 0, 0]  \n",
              "4  [3.6109179126442243, 0, 1, 0]  \n",
              "5  [3.6375861597263857, 0, 0, 1]  \n",
              "6  [3.5553480614894135, 0, 3, 0]  \n",
              "7  [3.6635616461296463, 0, 4, 0]  \n",
              "8   [4.219507705176107, 1, 8, 0]  \n",
              "9  [3.1354942159291497, 0, 0, 0]  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create a new column containing the features calculated as described above\n",
        "processed_df['text_features'] = [get_features_text(text) for text in processed_df['text']]\n",
        "processed_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yy6oXD2uiGnJ"
      },
      "outputs": [],
      "source": [
        "# extract X (text_featues) and y (label) from the dataset\n",
        "X = np.array(processed_df['text_features'].to_list())\n",
        "y_train = processed_df['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8qUB6GxAVx2",
        "outputId": "cd71c850-11a2-49f8-f03d-e1090554fc73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset inputs or features shape (num_samples, num_features): (90000, 4)\n",
            "Dataset outputs or labels shape (num_samples, ): 90000\n"
          ]
        }
      ],
      "source": [
        "# Check dataset shapes\n",
        "print(f\"Dataset inputs or features shape (num_samples, num_features): {X.shape}\") # -> (90000, 4)\n",
        "assert X.shape == (90000, 4)\n",
        "print(f\"Dataset outputs or labels shape (num_samples, ): {len(y_train)}\") # -> 90000\n",
        "assert len(y_train) == 90000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phoIJhv7ak8I"
      },
      "source": [
        "### 2.2 Data Standardisation\n",
        "\n",
        "We need to standardise our data for the same reasons as discussed for simple logistic regression above, but since our training and test set are in different files we will package the data standardisation in a function, so we can reuse it: first to standardise the training set, and then for the test set during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KZ0C13sAas7w"
      },
      "outputs": [],
      "source": [
        "#We commented all print statements for clean code display -- uncomment as needed\n",
        "def standardise_features(X: np.ndarray):\n",
        "    \"\"\"\n",
        "      Standardise the features contained in the given dataset.\n",
        "\n",
        "      Args:\n",
        "          X (array): array with dimension (num_samples, num_features).\n",
        "\n",
        "      Returns:\n",
        "          array with dimension (num_samples, num_features): dataset with standardised features.\n",
        "    \"\"\"\n",
        "    # Check means and standard deviations for each features\n",
        "    means = X.mean(axis=0)\n",
        "    stds = X.std(axis=0)\n",
        "\n",
        "    #print(\"Means of features:\", means)\n",
        "    #print(\"Standard deviations of features:\", stds)\n",
        "\n",
        "    # If not standardised, apply Standard Scaling (Z-score standardisation)\n",
        "    if any(stds != 1) or any(means != 0):\n",
        "        scaler = StandardScaler()\n",
        "        standardised_X = scaler.fit_transform(X)\n",
        "        #after_means = X.mean(axis=0)\n",
        "        #after_stds = X.std(axis=0)\n",
        "        #print(\"\\nMeans of features after standardisation:\", after_means)\n",
        "        #print(\"Standard deviations of features after standardisation:\", after_stds)\n",
        "    else:\n",
        "        print(\"Dataset is already standardised.\")\n",
        "    return standardised_X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hWUhR-JreJCP"
      },
      "outputs": [],
      "source": [
        "X_standardised = standardise_features(X) #standardise train set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dn_srAwHWus"
      },
      "source": [
        "As we did before, we need to initialise the Bias term ($b$) and concatenate it to the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `not using X_standardised below`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0PL6xm-6mi6T"
      },
      "outputs": [],
      "source": [
        "# Add bias term to X\n",
        "X_train = np.c_[np.ones(X_standardised.shape[0]), X]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7cO91Kqhuks",
        "outputId": "c2f32fef-c1c2-4926-853f-ad00733e5b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape (num_samples x (num_features+bias)): (90000, 5)\n"
          ]
        }
      ],
      "source": [
        "# Check training sets shapes\n",
        "print(f\"Training set shape (num_samples x (num_features+bias)): {X_train.shape}\") # -> (90000, 5)\n",
        "assert X_train.shape == (90000, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SukxZloYHjlW"
      },
      "source": [
        "### 2.3 Logistic Function: Softmax Function definition\n",
        "\n",
        "Following the definition of Softmax function\n",
        "\n",
        "**$\\text{softmax}(Z_i) = \\frac{\\text{exp}(Z_i)}{\\sum_{j=1}^{K} \\text{exp}(Z_j)}$**\n",
        "\n",
        " implement a function that calculates the Softmax for a given Z.\n",
        "\n",
        " The sum of the exponentials ($\\sum_{j=1}^{K} \\text{exp}(Z_j)$) may lead to very large numbers, this could cause instability in calculating the gradients. To avoid this, we reduce each value $Z_i$ by subtracting from the largest value in the input ([Softmax Function Reference](https://en.wikipedia.org/wiki/Softmax_function)):\n",
        "\n",
        "\n",
        "\n",
        " **$\\text{softmax}(Z_i) = \\frac{\\text{exp}(Z_i - \\text{max}(Z))}{\\sum_{j=1}^{K} \\text{exp}(Z_j - \\text{max}(Z))}$**\n",
        "\n",
        " Hint: use numpy to calculate the exponential, the max and the sum.\n",
        "\n",
        " **Note:** As we will take the max and the sum along each row (that means we perform these operations on columns), this will result in a matrix with only one dimension instead of two, and becuase there are other operators (e.g. exponential) that don't change the number of dimensions, we need to keep the dimension when applying the max and the sum operations (Search for parameters in max and sum numpy reference that can keep the dimensions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2YKRlNfNHkDe"
      },
      "outputs": [],
      "source": [
        "def softmax(Z: np.ndarray):\n",
        "  \"\"\"\n",
        "    Softmax function that gives us the predictions of the given samples using our model.\n",
        "\n",
        "    Args:\n",
        "        Z (array): array with dimension (num_train_samples, num_class), each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "    Returns:\n",
        "        array with dimension (num_train_samples, num_class): array containing a prediction made by the model for each sample.\n",
        "  \"\"\"\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "  return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "  ## END OF YOUR CODE ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsooAXoi9pJ_"
      },
      "source": [
        "### 2.4 Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qw7PfewH1lX"
      },
      "source": [
        "#### 2.4.1 Initialise Weights \\( $w_1, w_2, \\ldots, w_n$ \\)\n",
        "\n",
        "We need to initialise 1 weight for each class in the dataset for each feature composing a sample in our dataset.\n",
        "\n",
        "First, you need to identify the number of classes, then you can inizialise the correct number of weights.\n",
        "\n",
        "You can initialise all the weights to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_kFqF_nH2RG",
        "outputId": "9477e598-92b1-4fa5-a1d7-eb10665635e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights Shape (num_features+bias, num_classes): (5, 3)\n"
          ]
        }
      ],
      "source": [
        "# Initialize weights\n",
        "\n",
        "# Before, we had a single vector of weights with dimension (6,)\n",
        "# Now we need 1 weight for each feature+bias and for each class in the dataset\n",
        "# obtaining a matrix with dimensions (num_features+bias, num_classes).\n",
        "# The initialisation is similar to before, but we need to identify the number of classes\n",
        "# to initialise a matrix with the correct dimensions.\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "# num_classes = 3\n",
        "num_classes = len(np.unique(y_train))\n",
        "weights = np.zeros((X_train.shape[1], num_classes))\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "# Check weights shape\n",
        "print(f\"Weights Shape (num_features+bias, num_classes): {weights.shape}\") # -> (5, 3)\n",
        "assert weights.shape == (5, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8A2ahbpH6IU"
      },
      "source": [
        "#### 2.4.2 Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3dnBeYalH6Ys"
      },
      "outputs": [],
      "source": [
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.001\n",
        "num_iterations = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqQPtnNOIIp9"
      },
      "source": [
        "#### 2.4.3 Training Loop\n",
        "\n",
        "In each training iteration, we need to perform different steps:\n",
        "\n",
        "1.   Calculate Z\n",
        "2.   Use Z to get the predictions of the samples using our model;\n",
        "3.   Compute the error (loss function) between the predictions and the real labels in the dataset (*y_train*);\n",
        "4.   Calculate the gradient based on the obtained error;\n",
        "5.   Update the weights.\n",
        "\n",
        "You may use numpy functions [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) and [np.eye](https://numpy.org/devdocs/reference/generated/numpy.eye.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "Mk3By3BSIFEO",
        "outputId": "0c2550c4-a2c4-4d1f-ffcf-90331a1b4e52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 15/5000 [00:00<00:34, 144.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z Shape (num_train_samples, num_class): (90000, 3)\n",
            "Predictions Shape (num_train_samples, num_class): (90000, 3)\n",
            "Error Shape (num_train_samples, ): (90000, 3)\n",
            "Gradient Shape (num_features+bias, num_class): (5, 3)\n",
            "Weights Shape (num_features+bias, num_class): (5, 3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 5000/5000 [00:34<00:00, 143.61it/s]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Training the model\n",
        "# You can comment the print functions for a clear output after you complete coding all the training steps.\n",
        "losses = []\n",
        "for i in tqdm(range(num_iterations), total=num_iterations, desc=\"Training\"):\n",
        "\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  # 1. Calculate Z\n",
        "  # X_train: (num_train_samples, num_features+bias) x weights: (num_features+bias, num_class) -> Z: (num_train_samples, num_class)\n",
        "  Z = np.dot(X_train, weights)\n",
        "  if i==0:\n",
        "    print('Z Shape (num_train_samples, num_class):', Z.shape) # -> (90000, 3)\n",
        "    assert Z.shape == (90000, 3)\n",
        "\n",
        "  # 2. Get predictions\n",
        "  # Z: (num_train_samples, num_class) -> predictions: (num_train_samples, num_class)\n",
        "  predictions = softmax(Z)\n",
        "  if i==0:\n",
        "    print('Predictions Shape (num_train_samples, num_class):', predictions.shape) # -> (90000, 3)\n",
        "    assert predictions.shape == (90000, 3)\n",
        "\n",
        "  # 3. Compute error between prediction and real label\n",
        "  '''\n",
        "  The prediction for each data item is a vector of probabilites summing to 1.\n",
        "  For example, if we have a prediction for a data point like this: [0.2, 0.5, 0.3],\n",
        "  we can read this as there is 0.2 probability that the data item belongs to the first class,\n",
        "  0.5 to the second class, and 0.3 to the third class. Our gold label is just the index\n",
        "  of the class, taking the values 0, 1 or 2. Let's assume our gold label is 2.\n",
        "  To compare our predictions to the label, we need to represent the label in a comparable format.\n",
        "  To achieve this, we can use one-hot encoding for the labels (setting 1 in the desired index and 0 elsewhere),\n",
        "  giving the following for a label of 2: [0, 0, 1].\n",
        "  This we can compare to the prediction, finding that we are off by 0.7 from the perfect true class probability (1).\n",
        "\n",
        "  Hint: to get the one-hot encoding of the labels (y_train in our case), think about using np.eye() function\n",
        "  '''\n",
        "  # predictions: (num_train_samples, num_class) y_train: (num_train_samples, ) -> errors: (num_train_samples, num_class)\n",
        "  y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "  errors = predictions - y_train_one_hot\n",
        "  if i==0:\n",
        "    print('Error Shape (num_train_samples, ):', errors.shape)  # -> (90000, 3)\n",
        "    assert errors.shape == (90000, 3)\n",
        "\n",
        "  # 4. Calculate gradient\n",
        "  #  We transpose X_train to align the dimensions correctly with the error dimension for matrix multiplication.\n",
        "  # X_train.T: (num_features+bias, num_train_samples) x error: (num_train_samples, num_class) -> gradient or change in weights vector: (num_features+bias, num_class)\n",
        "  gradients = np.dot(X_train.T, errors)\n",
        "  if i==0:\n",
        "    print('Gradient Shape (num_features+bias, num_class):', gradients.shape)  # -> (5, 3)\n",
        "    assert gradients.shape == (5, 3)\n",
        "\n",
        "  # 5. Update weights\n",
        "  # gradient: (num_features+bias, num_class) -> weights: (num_features+bias, num_class)\n",
        "  weights -= learning_rate * gradients\n",
        "  if i==0:\n",
        "    print('Weights Shape (num_features+bias, num_class):', weights.shape) # -> (5, 3)\n",
        "    assert weights.shape == (5, 3)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  loss = -np.sum(y_train_one_hot * np.log(predictions+ 1e-15)) / len(y_train) # Add small epsilon to avoid log(0)\n",
        "  losses.append(loss)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm3U5KtGsV-g"
      },
      "source": [
        "The shape of the loss function plotted is not decreasing as we observed in Section 1 because the features we extracted from the texts are not representative enough to allow the model to learn proper decision boundaries to select the correct label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUOyqWzOIKcs"
      },
      "source": [
        "### 2.5 Evaluate the trained model\n",
        "\n",
        "As we did in Section 1.5, we want to evaluate the trained model to check the performance.\n",
        "\n",
        "1. We preprocess the test set as we did for the train set.\n",
        "2. We use the final weights obtained from training and we compute the predictions of our test set.\n",
        "3. We compare the predictions against the target labels to calculate the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nI7nhG41zg66",
        "outputId": "30101277-4f0a-4611-e9f1-09b318b2d788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set shape (num_test_samples , (num_features+bias)): (5700, 5)\n"
          ]
        }
      ],
      "source": [
        "def prepocess_data(filename):\n",
        "  \"\"\"\n",
        "    Preprocess the given dataset to set it in the correct format, i.e.\n",
        "    containing 2 columns (label, text_features) with standardised text features and adding bias to the features.\n",
        "    This function collects all the steps described in details in Sections 2.1 and 2.2\n",
        "\n",
        "    Args:\n",
        "        filename (string): name of the file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        X: bias+features extracted from the samples in the dataset.\n",
        "        y: gold labels of the dataset.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename, header=None)\n",
        "  df = df[df[0] != 1]\n",
        "  df['label'] = df[0]-2\n",
        "  df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "  df = df.drop(columns=[0,1,2])\n",
        "\n",
        "  df['text_features'] = [get_features_text(text) for text in df['text']]\n",
        "\n",
        "  X = np.array(df['text_features'].to_list())\n",
        "  X_standardised = standardise_features(X) #standardise test set\n",
        "  X = np.c_[np.ones(X_standardised.shape[0]), X_standardised]\n",
        "\n",
        "  y = df['label'].to_list()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X_test, y_test = prepocess_data('agnews_test.csv')\n",
        "\n",
        "print(f\"Test set shape (num_test_samples , (num_features+bias)): {X_test.shape}\") # -> (5700, 5)\n",
        "assert X_test.shape == (5700, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Egw08cIN1M",
        "outputId": "19193618-3704-4b93-ccfb-e586b1a76c73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5184210526315789\n"
          ]
        }
      ],
      "source": [
        "# Making predictions on the test set using trained weights\n",
        "# X_test: (num_test_samples, num_features) x weights: (num_features, num_class) -> Z: (num_samples, num_class)\n",
        "z = np.dot(X_test, weights)\n",
        "test_predictions = softmax(z)\n",
        "\n",
        "# select the class with the highest probability\n",
        "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDj_byJ4EMQc"
      },
      "source": [
        "## 3. Identify new text features (__Week 1 Submission Exercise__)\n",
        "\n",
        "Using the code from Section 2 on Multinomial Logistic Regression as a starting point, create new text features for the same multi-class task and try to obtain better performance.\n",
        "\n",
        "You can modify the features in the code provided in Section 2.1, in function `get_features_text`.\n",
        "\n",
        "Copy all and only the required code to a new notebook, and submit it for Week 1. Make sure you print both original performance (with the features used in Section 2) and performance for your new features to screen.\n",
        "\n",
        "Consult the module handbook for instructions how to share the weekly submission exercises with us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnF13fbPIark"
      },
      "source": [
        "# Resources\n",
        "\n",
        "- Speech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin, [Chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
        "- [Enchanted Learning - Wordlist](https://www.enchantedlearning.com/wordlist/)\n",
        "- [AG News dataset](https://huggingface.co/datasets/wangrongsheng/ag_news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHFAlkors5Z8"
      },
      "source": [
        "### 3.1 Creating new features\n",
        "In order to improve performance we need to hand design more features that can help in classifying whether the text is part of one class more than another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yl5F8Lt5XiJ2"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XWzN3nBPcWp50f_DjC2rpS7G2PvzjvmB' -O business.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1u3KQGgkFTN8s4fTGzJJOsmR4MGVlusp1' -O science.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1g_gYaij_xn1HwGebruK3JJRZy30P7E49' -O sports.txt\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "NfdPy_0cW36E",
        "outputId": "8729d9a4-c547-437e-d7a4-783e85e9f6bc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# create a new column containing the features calculated as described above\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m processed_df[\u001b[33m'\u001b[39m\u001b[33mtext_features\u001b[39m\u001b[33m'\u001b[39m] = [\u001b[43mget_features_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m processed_df[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     68\u001b[39m processed_df.head(\u001b[32m10\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# extract X (text_featues) and y (label) from the dataset\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mget_features_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     56\u001b[39m   word = word.strip().lower()\n\u001b[32m     57\u001b[39m   \u001b[38;5;66;03m# check if the word is present in the list of words associated to the current label\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m label_words:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# if it's present add 1 to the current count\u001b[39;00m\n\u001b[32m     60\u001b[39m     label_count += \u001b[32m1\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# append the final count to the features list\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# labels contained in the dataset\n",
        "labels = ['sports', 'business', 'science']\n",
        "\n",
        "# dataset is saved in a CSV file with no header and each column in separated by comma\n",
        "# the file has the following structure:\n",
        "# gold_label , title , body\n",
        "df = pd.read_csv('agnews_train.csv', header=None)\n",
        "\n",
        "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
        "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
        "df = df[df[0] != 1]\n",
        "\n",
        "# we create a 'label' column, subtract 2 from each gold label so we obtain a direct mapping with our list of labels:\n",
        "# 0 -> sports ; 1 -> business ; 2 -> science\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "# we concatenate title and body to obtain a unified text\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "# remove the original 3 columns to obtain our final processed dataset, containing 2 columns: label and text\n",
        "# the original 3 columns are the unmatched labels, the titles and the bodies\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)\n",
        "\n",
        "# computes the features of the given text and returns a list of features in the format:\n",
        "# [ln text_len, # sports words, # business words, # science words]\n",
        "def get_features_text(text: str):\n",
        "  \"\"\"\n",
        "    Calculated text features for the given text.\n",
        "\n",
        "    Args:\n",
        "        text (string): string containing the text.\n",
        "\n",
        "    Returns:\n",
        "        List with dimension 4: list containing the features extracted from the text in the order\n",
        "                                [ln text_len, # sports words, # business words, # science words].\n",
        "    \"\"\"\n",
        "  import math\n",
        "  # divide the text into words\n",
        "  words = text.split()\n",
        "\n",
        "  # initialise the features list adding the ln of the text length\n",
        "  features = [math.log(len(words))]\n",
        "\n",
        "  # for each target label we have in the dataset\n",
        "  for label in labels:\n",
        "    # open the list of words of the current label\n",
        "    with open(f'{label}.txt', 'r') as label_file:\n",
        "      # get all the words associated to the current label\n",
        "      # the file contains 1 word for each line, so we split the whole text with \\n\n",
        "      label_words = label_file.read().split('\\n')\n",
        "    label_count = 0\n",
        "    # for each word in the text,\n",
        "    for word in words:\n",
        "      word = word.strip().lower()\n",
        "      # check if the word is present in the list of words associated to the current label\n",
        "      if word in label_words:\n",
        "        # if it's present add 1 to the current count\n",
        "        label_count += 1\n",
        "    # append the final count to the features list\n",
        "    features.append(label_count)\n",
        "  # return the features list [ln text_len, # sports words, # business words, # science words]\n",
        "  return features\n",
        "\n",
        "# create a new column containing the features calculated as described above\n",
        "processed_df['text_features'] = [get_features_text(text) for text in processed_df['text']]\n",
        "processed_df.head(10)\n",
        "\n",
        "# extract X (text_featues) and y (label) from the dataset\n",
        "X = np.array(processed_df['text_features'].to_list())\n",
        "y_train = processed_df['label'].to_list()\n",
        "\n",
        "# Check dataset shapes\n",
        "print(f\"Dataset inputs or features shape (num_samples, num_features): {X.shape}\") # -> (90000, 4)\n",
        "assert X.shape == (90000, 4)\n",
        "print(f\"Dataset outputs or labels shape (num_samples, ): {len(y_train)}\") # -> 90000\n",
        "assert len(y_train) == 90000\n",
        "\n",
        "#We commented all print statements for clean code display -- uncomment as needed\n",
        "def standardise_features(X: np.ndarray):\n",
        "    \"\"\"\n",
        "      Standardise the features contained in the given dataset.\n",
        "\n",
        "      Args:\n",
        "          X (array): array with dimension (num_samples, num_features).\n",
        "\n",
        "      Returns:\n",
        "          array with dimension (num_samples, num_features): dataset with standardised features.\n",
        "    \"\"\"\n",
        "    # Check means and standard deviations for each features\n",
        "    means = X.mean(axis=0)\n",
        "    stds = X.std(axis=0)\n",
        "\n",
        "    #print(\"Means of features:\", means)\n",
        "    #print(\"Standard deviations of features:\", stds)\n",
        "\n",
        "    # If not standardised, apply Standard Scaling (Z-score standardisation)\n",
        "    if any(stds != 1) or any(means != 0):\n",
        "        scaler = StandardScaler()\n",
        "        standardised_X = scaler.fit_transform(X)\n",
        "        #after_means = X.mean(axis=0)\n",
        "        #after_stds = X.std(axis=0)\n",
        "        #print(\"\\nMeans of features after standardisation:\", after_means)\n",
        "        #print(\"Standard deviations of features after standardisation:\", after_stds)\n",
        "    else:\n",
        "        print(\"Dataset is already standardised.\")\n",
        "    return standardised_X\n",
        "\n",
        "X_standardised = standardise_features(X) #standardise train set\n",
        "\n",
        "# Add bias term to X\n",
        "X_train = np.c_[np.ones(X_standardised.shape[0]), X]\n",
        "\n",
        "# Check training sets shapes\n",
        "print(f\"Training set shape (num_samples x (num_features+bias)): {X_train.shape}\") # -> (90000, 5)\n",
        "assert X_train.shape == (90000, 5)\n",
        "\n",
        "def softmax(Z: np.ndarray):\n",
        "  \"\"\"\n",
        "    Softmax function that gives us the predictions of the given samples using our model.\n",
        "\n",
        "    Args:\n",
        "        Z (array): array with dimension (num_train_samples, num_class), each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "    Returns:\n",
        "        array with dimension (num_train_samples, num_class): array containing a prediction made by the model for each sample.\n",
        "  \"\"\"\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "  return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  # Initialize weights\n",
        "\n",
        "# Before, we had a single vector of weights with dimension (6,)\n",
        "# Now we need 1 weight for each feature+bias and for each class in the dataset\n",
        "# obtaining a matrix with dimensions (num_features+bias, num_classes).\n",
        "# The initialisation is similar to before, but we need to identify the number of classes\n",
        "# to initialise a matrix with the correct dimensions.\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "num_classes = len(np.unique(y_train))\n",
        "weights = np.zeros((X_train.shape[1], num_classes))\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "# Check weights shape\n",
        "print(f\"Weights Shape (num_features+bias, num_classes): {weights.shape}\") # -> (5, 3)\n",
        "assert weights.shape == (5, 3)\n",
        "\n",
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.001\n",
        "num_iterations = 5000\n",
        "\n",
        "# ------------------------------------\n",
        "'''new code'''\n",
        "# ------------------------------------\n",
        "# Training the model\n",
        "# You can comment the print functions for a clear output after you complete coding all the training steps.\n",
        "losses = []\n",
        "for i in tqdm(range(num_iterations), total=num_iterations, desc=\"Training\"):\n",
        "\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  # 1. Calculate Z\n",
        "  # X_train: (num_train_samples, num_features+bias) x weights: (num_features+bias, num_class) -> Z: (num_train_samples, num_class)\n",
        "  Z = np.dot(X_train, weights)\n",
        "  if i==0:\n",
        "    print('Z Shape (num_train_samples, num_class):', Z.shape) # -> (90000, 3)\n",
        "    assert Z.shape == (90000, 3)\n",
        "\n",
        "  # 2. Get predictions\n",
        "  # Z: (num_train_samples, num_class) -> predictions: (num_train_samples, num_class)\n",
        "  predictions = softmax(Z)\n",
        "  if i==0:\n",
        "    print('Predictions Shape (num_train_samples, num_class):', predictions.shape) # -> (90000, 3)\n",
        "    assert predictions.shape == (90000, 3)\n",
        "\n",
        "  # 3. Compute error between prediction and real label\n",
        "  '''\n",
        "  The prediction for each data item is a vector of probabilites summing to 1.\n",
        "  For example, if we have a prediction for a data point like this: [0.2, 0.5, 0.3],\n",
        "  we can read this as there is 0.2 probability that the data item belongs to the first class,\n",
        "  0.5 to the second class, and 0.3 to the third class. Our gold label is just the index\n",
        "  of the class, taking the values 0, 1 or 2. Let's assume our gold label is 2.\n",
        "  To compare our predictions to the label, we need to represent the label in a comparable format.\n",
        "  To achieve this, we can use one-hot encoding for the labels (setting 1 in the desired index and 0 elsewhere),\n",
        "  giving the following for a label of 2: [0, 0, 1].\n",
        "  This we can compare to the prediction, finding that we are off by 0.7 from the perfect true class probability (1).\n",
        "\n",
        "  Hint: to get the one-hot encoding of the labels (y_train in our case), think about using np.eye() function\n",
        "  '''\n",
        "  # predictions: (num_train_samples, num_class) y_train: (num_train_samples, ) -> errors: (num_train_samples, num_class)\n",
        "  y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "  errors = predictions - y_train_one_hot\n",
        "  if i==0:\n",
        "    print('Error Shape (num_train_samples, ):', errors.shape)  # -> (90000, 3)\n",
        "    assert errors.shape == (90000, 3)\n",
        "\n",
        "  # 4. Calculate gradient\n",
        "  #  We transpose X_train to align the dimensions correctly with the error dimension for matrix multiplication.\n",
        "  # X_train.T: (num_features+bias, num_train_samples) x error: (num_train_samples, num_class) -> gradient or change in weights vector: (num_features+bias, num_class)\n",
        "  gradients = np.dot(X_train.T, errors)\n",
        "  if i==0:\n",
        "    print('Gradient Shape (num_features+bias, num_class):', gradients.shape)  # -> (5, 3)\n",
        "    assert gradients.shape == (5, 3)\n",
        "\n",
        "  # 5. Update weights\n",
        "  # gradient: (num_features+bias, num_class) -> weights: (num_features+bias, num_class)\n",
        "  weights -= learning_rate * gradients\n",
        "  if i==0:\n",
        "    print('Weights Shape (num_features+bias, num_class):', weights.shape) # -> (5, 3)\n",
        "    assert weights.shape == (5, 3)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  loss = -np.sum(y_train_one_hot * np.log(predictions+ 1e-15)) / len(y_train) # Add small epsilon to avoid log(0)\n",
        "  losses.append(loss)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "\n",
        "def prepocess_data(filename):\n",
        "  \"\"\"\n",
        "    Preprocess the given dataset to set it in the correct format, i.e.\n",
        "    containing 2 columns (label, text_features) with standardised text features and adding bias to the features.\n",
        "    This function collects all the steps described in details in Sections 2.1 and 2.2\n",
        "\n",
        "    Args:\n",
        "        filename (string): name of the file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        X: bias+features extracted from the samples in the dataset.\n",
        "        y: gold labels of the dataset.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename, header=None)\n",
        "  df = df[df[0] != 1]\n",
        "  df['label'] = df[0]-2\n",
        "  df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "  df = df.drop(columns=[0,1,2])\n",
        "\n",
        "  df['text_features'] = [get_features_text(text) for text in df['text']]\n",
        "\n",
        "  X = np.array(df['text_features'].to_list())\n",
        "  X_standardised = standardise_features(X) #standardise test set\n",
        "  X = np.c_[np.ones(X_standardised.shape[0]), X_standardised]\n",
        "\n",
        "  y = df['label'].to_list()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X_test, y_test = prepocess_data('agnews_test.csv')\n",
        "\n",
        "print(f\"Test set shape (num_test_samples , (num_features+bias)): {X_test.shape}\") # -> (5700, 5)\n",
        "assert X_test.shape == (5700, 5)\n",
        "\n",
        "# Making predictions on the test set using trained weights\n",
        "# X_test: (num_test_samples, num_features) x weights: (num_features, num_class) -> Z: (num_samples, num_class)\n",
        "z = np.dot(X_test, weights)\n",
        "test_predictions = softmax(z)\n",
        "\n",
        "# select the class with the highest probability\n",
        "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOKFTf3peYwE"
      },
      "source": [
        "current accuracy = 0.48473"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
