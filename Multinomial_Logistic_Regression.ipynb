{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CA6011 Deep Learning for NLP: Week 1 Lab - Logistic Regression\n",
        "\n",
        "**Logistic Regression** (sometimes called logit regression) estimates the parameters of a logistic model. In **binary logistic regression** there is a single dependent variable; in **multinomial logistic regression**, there are multiple dependent variables.\n",
        "\n",
        "In this lab you will code, more or less from scratch, first (standard) logistic regression, and then multinomial logistic regression.\n",
        "\n",
        "We provide a coding framework with a lot of the code in place, for you to add to. At two points during the lab, we will share partial solutions: after 45mins and after 90mins.\n",
        "\n",
        "The remainder of the lab (for which we don't provide solutions) is for you to complete on your own, and must be submitted for assessment for Week 1 of this module (see module handbook).\n",
        "\n"
      ],
      "metadata": {
        "id": "ranSpOffE3VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Troughout this notebook, additional information can be found in the Week 1 lecture slides and Chapter 4 of the book **[Speech and Language Processing (3rd ed. draft) by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/4.pdf)**."
      ],
      "metadata": {
        "id": "px72Z8CKj3nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A8M-G8P2Ej5A"
      },
      "outputs": [],
      "source": [
        "# %matplotlib inline allows for displaying plots directly in the Jupyter notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# NumPy is a library for numerical computations, with support for arrays and matrices\n",
        "import numpy as np\n",
        "\n",
        "# tqdm is used for creating progress bars to track the progress of for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Matplotlib is a plotting library, and we use it for tracking the loss values across iterations\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# make_classification is a function in the scikit-learn library to generate a random classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# StandardScaler is a function in scikit-learn for standardising (scaling) the features of a dataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# train_test_split is a function in scikit-learn for splitting a dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# accuracy_score is a function in scikit-learn that implements the accuracy metric to evaluate classification models\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# IPython.display allows clearing the output in a Jupyter notebook cell\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Seed for reproducibility, fixing the random seed ensures consistent results when rerunning the code\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1  Logistic Regression\n",
        "\n",
        "$\\text{Decision}(x) = \\begin{cases}\n",
        "1 & \\text{if } P(y = 1|x) > 0.5 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}$\n",
        "\n",
        "\n",
        "**$z = b + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n$**\n",
        "\n",
        "**$P(y = 1) = \\frac{1}{1 + e^{-Z}}$** or $\\frac{1}{1 + e^{-(b + w_1 x_1 + \\ldots + w_n x_n)}}$\n",
        "\n",
        "- $ x_1, x_2, \\ldots, x_n $ are the input features.\n",
        "- $b$ is the bias term.\n",
        "- $w_1, w_2, \\ldots, w_n$ are the corresponding weights.\n",
        "- $\\frac{1}{1 + e^{-Z}}$ or $\\frac{1}{1 + e^{-(b + w_1 x_1 +  \\ldots + w_n x_n)}}$ is the sigmoid function.\n"
      ],
      "metadata": {
        "id": "q1KOsyJuFK1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we develop an implementation of logistic regression and apply it to a synthetic dataset."
      ],
      "metadata": {
        "id": "QcvaqGkhFWae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Dataset creation\n",
        "First, we need to create a synthetic dataset to train the logistic regression. The dataset will contain **1000 samples**, or data points, where each sample has **5 features** classified into **2 classes**.\n",
        "\n",
        "`make_classification` generates a random *n*-class classification problem; for more information, see its scikit-learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."
      ],
      "metadata": {
        "id": "m0qZyboJFiq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=2, random_state=seed)"
      ],
      "metadata": {
        "id": "bzBbZ9wRFKHe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`X` holds the features for each data point or sample.\n",
        "\n",
        "`y` holds a class label for each `X`.\n",
        "\n",
        "In this context, `X` serves as the input for our regression model, while `y` represents the desired output from the model.\n",
        "\n",
        "Refer to the lecture slides for explanation regarding the shapes of the different matrices, and the importance of getting shapes and order right."
      ],
      "metadata": {
        "id": "bcORQiN8dYGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset shapes\n",
        "print(f\"Dataset inputs or features shape (num_samples, num_features): {X.shape}\") # -> (1000, 5)\n",
        "assert X.shape == (1000,5)  # checks whether the condition is True and returns an error if it's False\n",
        "print(f\"Dataset outputs or labels shape (num_samples, ): {y.shape}\")  # -> (1000,)\n",
        "assert y.shape == (1000,)"
      ],
      "metadata": {
        "id": "hx5FncXVhK6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310e8171-067f-40d3-ef51-fae3d89303ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset inputs or features shape (num_samples, num_features): (1000, 5)\n",
            "Dataset outputs or labels shape (num_samples, ): (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 data points and their class labels\n",
        "for i in range(10):\n",
        "    print(f\"Data Point {i + 1}: Features = {X[i]}, Class Label = {y[i]}\")"
      ],
      "metadata": {
        "id": "DNK1NOazcQjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d503a6-a4c4-41af-d58b-0e880b4f10f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Point 1: Features = [-0.43964281  0.54254734 -0.82241993  0.40136622 -0.85484   ], Class Label = 0\n",
            "Data Point 2: Features = [ 2.82223116 -2.48085944 -1.14769139 -2.10113103  3.04027792], Class Label = 1\n",
            "Data Point 3: Features = [ 1.61838572 -1.36947785 -2.08411294 -1.17965857  1.61360231], Class Label = 1\n",
            "Data Point 4: Features = [ 1.65904812 -0.61520205  1.11268837 -0.83509772 -0.27220548], Class Label = 1\n",
            "Data Point 5: Features = [ 1.84982402 -1.67945551 -0.92669831 -1.40250885  2.12312866], Class Label = 1\n",
            "Data Point 6: Features = [ 0.07711125  0.23716016  0.58441298  0.0872754  -0.66175283], Class Label = 0\n",
            "Data Point 7: Features = [-0.38875395  0.56336538  0.01427307  0.39458146 -0.96012797], Class Label = 0\n",
            "Data Point 8: Features = [ 2.04095851 -0.5083624  -1.77623523 -0.90945217 -0.94172241], Class Label = 1\n",
            "Data Point 9: Features = [ 1.05244054 -0.04826718 -0.91750282 -0.36749269 -1.00799498], Class Label = 1\n",
            "Data Point 10: Features = [ 0.76301635 -0.15836982  0.88688739 -0.32496838 -0.42944873], Class Label = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Data standardisation\n",
        "Next, we check whether our data is already standardised or requires scaling to ensure that feature values have similar ranges.\n",
        "\n",
        "Scaling dataset features, or ensuring similar scales is important, particularly for gradient-based optimisation (gradient decent) used in logistic regression. Similar feature scales enable faster convergence during the optimisation process, crucial for finding the minimum of the cost function. Moreover, feature scaling enhances numerical stability, as logistic regression involves exponential functions (Sigmoid function), preventing issues with very large or small input values and ensuring a stable optimisation process.\n",
        "\n",
        "Refer to the lecture slides for more information regarding the importance of feature scaling.\n",
        "\n",
        "Here we use standardisation. We will employ `StandardScaler` (Z-score standardisation), which standardises the data to have a mean of 0 and a standard deviation of 1. There are other scaling techniques, e.g. see [here](https://developers.google.com/machine-learning/data-prep/transform/normalization).\n",
        "\n",
        "**Note:** We may not get exactly 0 mean or 1 standard deviation due to finite precision arithmetic in computers introducing small rounding errors. While these errors accumulate during calculations, their impact on the performance of the machine learning model (a regression model in our case) is typically negligible."
      ],
      "metadata": {
        "id": "0B5uScykNio_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check means and standard deviations for each feature\n",
        "means = X.mean(axis=0)\n",
        "stds = X.std(axis=0)\n",
        "\n",
        "print(\"Means of features:\", means)\n",
        "print(\"Standard deviations of features:\", stds)\n",
        "\n",
        "# If not standardised, apply Standard Scaling (Z-score standardisation)\n",
        "if any(stds != 1) or any(means != 0):\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    after_means = X.mean(axis=0)\n",
        "    after_stds = X.std(axis=0)\n",
        "    print(\"\\nMeans of features after standardisation:\", after_means)\n",
        "    print(\"Standard deviations of features after standardisation:\", after_stds)\n",
        "else:\n",
        "    print(\"Dataset is already standardised.\")"
      ],
      "metadata": {
        "id": "I14aisGmNmqk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3f861f-2fe5-4d8b-d14e-c273ef72d133"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Means of features: [-0.02254529  0.02970401  0.00875815  0.02147522 -0.04843296]\n",
            "Standard deviations of features: [1.40325417 0.85853954 0.98707998 0.81769263 1.32147592]\n",
            "\n",
            "Means of features after standardisation: [-1.11022302e-17 -9.32587341e-18  2.63122857e-17 -3.46389584e-17\n",
            " -3.77475828e-18]\n",
            "Standard deviations of features after standardisation: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we need to initialise the Bias term ($b$) and concatenate it to the dataset.\n",
        "\n",
        "In logistic regression, the bias term is represented by the intercept, serving as a term shared by all data points. To make the bias term learnable (allow gradient descent to find its value), we put in on a par with the input features, concatenating a column of ones to the feature matrix `X` as a constant feature (i.e. same feature value for all data points). This addition allows the model to learn a weight for this constant feature, ensuring the same value for all data points."
      ],
      "metadata": {
        "id": "ACGLBHsr1_Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add bias term to X:\n",
        "#np.ones(X.shape[0]): This creates a column vector of ones with the same number of rows as X\n",
        "#np.c_: This concatenates the column vectors of the first matrix with the second matrix.\n",
        "#np.c_[np.ones(X.shape[0]), X]: This concatenates the one column of the first matrix, np.ones(X.shape[0]), with the second matrix, X.\n",
        "\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print('X Shape:', X.shape)  # -> (1000, 6)\n",
        "assert X.shape == (1000, 6)"
      ],
      "metadata": {
        "id": "6UcFuptIF2AG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309c843d-e058-472b-a9c2-3d16457b3ce4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Shape: (1000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 items in the synthetic dataset and their class labels after adding the bias term\n",
        "for i in range(10):\n",
        "    print(f\"Data Item {i + 1}: Features = {X[i]}, Class Label = {y[i]}\")"
      ],
      "metadata": {
        "id": "U-ZFYzEklFlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a578989c-f6a0-4fde-b850-a5bec4b955fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Item 1: Features = [ 1.         -0.2972359   0.59734387 -0.84205748  0.464589   -0.61023211], Class Label = 0\n",
            "Data Item 2: Features = [ 1.          2.02727098 -2.92422578 -1.17158646 -2.59584857  2.33731909], Class Label = 1\n",
            "Data Item 3: Features = [ 1.          1.16937548 -1.62972326 -2.12026495 -1.46893067  1.25771136], Class Label = 1\n",
            "Data Item 4: Features = [ 1.          1.19835269 -0.7511664   1.11837971 -1.0475488  -0.1693353 ], Class Label = 1\n",
            "Data Item 5: Features = [ 1.          1.33430518 -1.99077555 -0.94770077 -1.74146618  1.64328505], Class Label = 1\n",
            "Data Item 6: Features = [ 1.          0.07101817  0.24163843  0.58318965  0.08047056 -0.46411733], Class Label = 0\n",
            "Data Item 7: Features = [ 1.         -0.26097101  0.62159207  0.0055871   0.45629155 -0.68990664], Class Label = 0\n",
            "Data Item 8: Features = [ 1.          1.47051322 -0.62672293 -1.80835739 -1.13848083 -0.6759786 ], Class Label = 1\n",
            "Data Item 9: Features = [ 1.          0.76606637 -0.0908184  -0.93838492 -0.47568963 -0.72612902], Class Label = 1\n",
            "Data Item 10: Features = [ 1.          0.55981422 -0.21906251  0.88962319 -0.42368438 -0.28832593], Class Label = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we split the dataset into train and test, so that we can use the train set to train and update the weights of our model and the test set to evaluate the performance of our final model.\n",
        "\n",
        "We divide the dataset into 80% train and 20% test.\n",
        "\n",
        "For more information see the documentation on [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) function."
      ],
      "metadata": {
        "id": "b9tmHeyy2PuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "\n",
        "# Check training/test sets shapes\n",
        "print(f\"Training set shape (num_train_samples , (num_features+bias)): {X_train.shape}\") # -> (800, 6)\n",
        "assert X_train.shape == (800, 6)\n",
        "print(f\"Test set shape (num_test_samples , (num_features+bias)): {X_test.shape}\") # -> (200, 6)\n",
        "assert X_test.shape == (200, 6)"
      ],
      "metadata": {
        "id": "7GanFaoJGQ2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6e5a2c-905b-4833-e971-037a7243e675"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape (num_train_samples , (num_features+bias)): (800, 6)\n",
            "Test set shape (num_test_samples , (num_features+bias)): (200, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Logistic Function: Sigmoid Function definition\n",
        "Following the definition of Sigmoid function\n",
        "\n",
        "> **$P (y = 1) = \\frac{1}{1 + e^{-z}}$**\n",
        "\n",
        " we implement a function that calculates the Sigmoid for a given $z$, where\n",
        "\n",
        " **$z = b + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n$**.\n",
        "\n",
        " Insert your own code below. Hint: use `numpy.exp` to calculate the exponential (see [here](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
      ],
      "metadata": {
        "id": "G5VMsVOuFr_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z: np.ndarray):\n",
        "  \"\"\"\n",
        "    Sigmoid function that gives us the predictions of the given samples using our model.\n",
        "\n",
        "    Args:\n",
        "        Z (array): array with dimension (num_samples, ), each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "    Returns:\n",
        "        array with dimension (num_samples, ): array containing a prediction made by the model for each sample.\n",
        "  \"\"\"\n",
        "  return 1 / (1 + np.exp(-Z))"
      ],
      "metadata": {
        "id": "GNdErVp7FrC3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Training Phase\n",
        "In this section, we define the training procedure."
      ],
      "metadata": {
        "id": "1uHrCKlBGRk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.1 Initialise Weights \\( $w_1, w_2, \\ldots, w_n$ \\)\n",
        "\n",
        "We need to initialise one weight for each dataset feature, giving a vector of length num_features + 1 (for the bias).\n",
        "\n",
        "Insert your own code below, to initialise all the weights to 0."
      ],
      "metadata": {
        "id": "VKTtffg-F2ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise weights (one for each feature)\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "weights = np.zeros(X.shape[1])\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "# Check weights shape\n",
        "print(f\"Weights Shape (num_features+bias, ): {weights.shape}\")  # -> (6,)\n",
        "assert weights.shape == (6,)"
      ],
      "metadata": {
        "id": "G4zXTPX9F2q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d758446-5b4e-42f6-8486-4887cc08167e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Shape (num_features+bias, ): (6,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.2 Hyperparameters\n",
        "\n",
        "In order to train our Logistic Regression, we need to define the hyperparameters. In this case, we need to define:\n",
        "\n",
        "1.   **Learning rate**, i.e. how much we update the weights at each iteration;\n",
        "2.   **Number of iterations**, i.e. how many times we iterate on the training dataset to update the weights.\n"
      ],
      "metadata": {
        "id": "QbRctnI1GPwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000"
      ],
      "metadata": {
        "id": "q9Ej3ZtTGI5X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4.3 Training Loop"
      ],
      "metadata": {
        "id": "ZRIT7J742l8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Cross Entropy Loss**\n",
        "\n",
        "To train our model, we need to have a loss function and understand how to update the weights of the model.\n",
        "\n",
        "As explained in the lecture, Cross Entropy loss $\\text{L}$ expresses, for an observation $x$, how close the classifier output is to the correct output ($y$, which is 0 or 1).\n",
        "\n",
        "Recall that the system's output is $\\hat{y} = \\sigma(w \\cdot x + b)$, where $\\sigma$ is the sigmoid function, and $z = w \\cdot x + b$ .\n",
        "\n",
        "So, we aim to find the parameters $w$, $b$ that minimize the negative log probability of the true $y$ labels in the training data given the observations $x$. As explained in the lecture, this gives us the cross entropy loss:\n",
        "\n",
        "$\\text{L}_{\\text{CE}}(\\hat{y}, y) = - [y \\log σ (w \\cdot x + b) + (1 - y) \\log (1 - \\sigma (w \\cdot x + b))]$\n",
        "\n",
        "By following the gradients of the loss function, we can get incrementally closer to the optimal solution (provided the learning rate is not too big), as it's associated with a convex loss surface.\n",
        "\n",
        "At any point during the learning process, the loss value tells us how good the model currently is.\n",
        "\n",
        "We incrementally update the weights on the basis of the loss function gradients.\n",
        "\n",
        "The gradients are calculated by taking the partial first derivative (giving the slope) of the Cross Entropy loss function with respect to each weight in turn (see lecture slides).\n",
        "\n",
        "Once we have the full vector of gradients *g*, we use it to update the weights as part of Stochastic Gradient Descent (Section 5.6 - Figure 5.6):\n",
        "\n",
        "$w = w - \\eta * g$,\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "It's important to identify the right learning rate because if we set it too high, we may skip the maximum point and if we set it too low we may end up in a local maximum."
      ],
      "metadata": {
        "id": "YXwa3mGDmJpP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training Steps**\n",
        "\n",
        "In each training iteration, we need to perform different steps:\n",
        "\n",
        "<ol>\n",
        "<li> Calculate $z$ as the product between the weights and the features of the samples:\n",
        "$z = b + w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n$\n",
        "<li> Use <code>Z</code> to get the predictions of the samples using our model (softmax function);\n",
        "<li> Use cross entropy loss to calculate the gradient (as the basis for the update in weights):\n",
        "  <ol type=a>\n",
        "  <li> Compute the difference (error) between the predictions and the real labels in the dataset (<code>y_train</code>) for each data point;\n",
        "  <li> Multiply each error with its corresponding input $x$;\n",
        "  </ol>\n",
        "<li> Update the weights vector.\n",
        "</ol>\n",
        "\n",
        "In our implementation, we use Batch Gradient Descent (using the full dataset) when updating weights. Refer to the lecture slides for details.\n",
        "\n",
        "Insert your code where indicated below. The only numpy function required is the [np.dot function](https://numpy.org/doc/stable/reference/generated/numpy.dot.html).\n",
        "\n",
        "Hint: make sure to keep track of the shapes of matrices. You can print out their shapes as we did above in order to check; however, to maintain a clean display of your code, print only the shapes of the first iteration (when i=0)."
      ],
      "metadata": {
        "id": "-Rv9KVJ8mC5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "losses = []\n",
        "# tqdm is a function that given a list, the total number of iterations and a description,\n",
        "# returns a progressbar showing the time execution\n",
        "for i in tqdm(range(num_iterations), total=num_iterations, desc=\"Training\"):\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "\n",
        "  # 1. Calculate Z\n",
        "  # X_train: (num_train_samples, num_features+bias) x weights: (num_features+bias, ) -> Z: (num_train_samples, )\n",
        "  Z = np.dot(X_train, weights)\n",
        "  if i==0:\n",
        "    print('Z Shape (num_train_samples, ):', Z.shape)  # -> (800,)\n",
        "    assert Z.shape == (800,)\n",
        "\n",
        "  # 2. Get the predictions\n",
        "  # Z: (num_train_samples, ) -> predictions: (num_train_samples, )\n",
        "  predictions = sigmoid(Z)\n",
        "  if i==0:\n",
        "    print('Predictions Shape (num_train_samples, ):', predictions.shape) # -> (800,)\n",
        "    assert predictions.shape == (800,)\n",
        "\n",
        "  # 3.1. Compute errors between predictions and real labels\n",
        "  # predictions: (num_train_samples, ) - y_train: (num_train_samples, ) -> errors: (num_train_samples, )\n",
        "  errors = predictions - y_train\n",
        "  if i==0:\n",
        "    print('Error Shape (num_train_samples, ):', errors.shape) # -> (800,)\n",
        "    assert errors.shape == (800,)\n",
        "\n",
        "  # 3.2. Calculate gradient\n",
        "  # we transpose X_train to align the dimensions correctly with the error dimension for matrix multiplication.\n",
        "  # X_train.T: (num_features+bias, num_train_samples) x error: (num_train_samples, ) -> gradient or change in weights vector: (num_features+bias, )\n",
        "  gradients = np.dot(X_train.T, errors)\n",
        "  if i==0:\n",
        "    print('Gradient Shape (num_features+bias, ):', gradients.shape) # -> (6,)\n",
        "    assert gradients.shape == (6,)\n",
        "\n",
        "  # 5. Update weights\n",
        "  # gradient: (num_features+bias, ) -> weights: (num_features+bias, )\n",
        "  weights -= learning_rate * gradients\n",
        "  if i==0:\n",
        "    print('Weights Shape (num_features+bias, ):', weights.shape) # -> (6,)\n",
        "    assert weights.shape == (6,)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  loss = (-y_train * np.log(predictions) - (1 - y_train) * np.log(1 - predictions)).mean()\n",
        "\n",
        "  losses.append(loss)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HvhK__vsGVR2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "bc9ccaff-ce8a-43a3-9d45-2b9a1dd01481"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z Shape (num_train_samples, ): (800,)\n",
            "Predictions Shape (num_train_samples, ): (800,)\n",
            "Error Shape (num_train_samples, ): (800,)\n",
            "Gradient Shape (num_features+bias, ): (6,)\n",
            "Weights Shape (num_features+bias, ): (6,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1000/1000 [00:00<00:00, 5160.02it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 Evaluate the trained model\n",
        "\n",
        "Finally, we want to evaluate the trained model on the test set to check the performance.\n",
        "\n",
        "We use the final weights obtained from training and we compute the predictions of our test set.\n",
        "\n",
        "We compare the predictions against the gold labels to calculate the accuracy of our model.\n",
        "\n",
        "Given the predictions of our model and the gold labels, Accuracy is defined as follows:\n",
        "\n",
        "$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + TN + FN}$\n",
        "\n",
        "- TP (True Positives): The number of instances that are actually positive (belong to the positive class) and are correctly predicted as positive by the model.\n",
        "\n",
        "- TN (True Negatives): The number of instances that are actually negative (belong to the negative class) and are correctly predicted as negative by the model.\n",
        "\n",
        "- FP (False Positives): The number of instances that are actually negative but are incorrectly predicted as positive by the model.\n",
        "\n",
        "- FN (False Negatives): The number of instances that are actually positive but are incorrectly predicted as negative by the model."
      ],
      "metadata": {
        "id": "-0cPObWEGq2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test set using trained weights\n",
        "# X_test: (num_test_samples, num_features) x weights: (num_features, ) -> Z: (num_samples, )\n",
        "z = np.dot(X_test, weights)\n",
        "test_predictions = sigmoid(z)\n",
        "\n",
        "'''\n",
        "Recall: Decision(x) = 1 if P(y = 1|x) > 0.5, Decision(x) = 0 otherwise\n",
        "np.round below will set any probability(test_predictions) greater than the threshold 0.5 to 1, and to 0 otherwise.\n",
        "'''\n",
        "binary_predictions = np.round(test_predictions)\n",
        "\n",
        "# Compute Accuracy using gold labels and predictions\n",
        "accuracy = accuracy_score(y_test, binary_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "PWiFtE3ZGm0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72c7ae7-fbf3-4dfa-cbfa-032cf5207081"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2  Multinomial Logistic Regression\n",
        "\n",
        "Standard logistic regression is for binary classification (there is one class, and instances either belong to it or they don't). **Multinomial Logistic Regression** is a generalisation to multi-class classification. Instead of using the Sigmoid function, it uses Softmax to obtain a probability distribution over the $K$ classes.\n",
        "\n",
        "Other than that, we also now need $K$ weight vectors and gradient vectors rather than single ones as in standard (binary) logistic regression.\n",
        "\n",
        "\n",
        "**Multinomial Logistic Regression modelling:**\n",
        "\n",
        "$P(Y_k=1 | X) = \\frac{\\text{exp}(W_k \\cdot X + b_k)}{\\sum_{j=1}^{K} \\text{exp}(W_j \\cdot X + b_j)}$\n",
        "\n",
        " - $K$ is the number of classes,\n",
        " - $\\frac{\\text{exp}(W_k \\cdot X + b_k)}{\\sum_{j=1}^{K} \\text{exp}(W_j \\cdot X + b_j)}$ is the softmax function\n",
        "\n",
        "\n",
        " Refer to the lecture slides for details."
      ],
      "metadata": {
        "id": "LOx-iPViHJTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we develop an implementation of multinomial logistic regression, and apply it to a real-world dataset."
      ],
      "metadata": {
        "id": "Mzq_MUvFHQ97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Dataset\n",
        "\n",
        "Instead of creating a toy dataset as we did in Section 1.1, in this section we'll use a real world dataset.\n",
        "\n",
        "We use the **[AGnews dataset](https://huggingface.co/datasets/wangrongsheng/ag_news)** composed of news articles categorised by domain. The dataset contains 4 domains: *World*; *Sports*, *Business*, and *Science/Technology*.\n",
        "\n",
        "The dataset is already divided into train and test sets containing 120K and 7.6K samples, respectively.\n",
        "\n",
        "For this exercise, we are going to exclude the World category.\n",
        "\n",
        "First, we download all the files composing the dataset."
      ],
      "metadata": {
        "id": "1Xke41_TIqf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "tS5Y0MuueGuX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are able to load the dataset and preprocess it. We use the labels *sports, business* and *science* for our three news domains."
      ],
      "metadata": {
        "id": "CvD2TWlRxZV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# labels contained in the dataset\n",
        "labels = ['sports', 'business', 'science']\n",
        "\n",
        "# dataset is saved in a CSV file with no header and each column in separated by comma\n",
        "# the file has the following structure:\n",
        "# gold_label , title , body\n",
        "df = pd.read_csv('agnews_train.csv', header=None)\n",
        "\n",
        "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
        "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
        "df = df[df[0] != 1]\n",
        "\n",
        "# we create a 'label' column, subtract 2 from each gold label so we obtain a direct mapping with our list of labels:\n",
        "# 0 -> sports ; 1 -> business ; 2 -> science\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "# we concatenate title and body to obtain a unified text\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "# remove the original 3 columns to obtain our final processed dataset, containing 2 columns: label and text\n",
        "# the original 3 columns are the unmatched labels, the titles and the bodies\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)"
      ],
      "metadata": {
        "id": "3NyGV3Nhev-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "ef66f27a-f02f-405e-e659-c83a69704e7f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      1  Wall St. Bears Claw Back Into the Black (Reute...\n",
              "1      1  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
              "2      1  Oil and Economy Cloud Stocks' Outlook (Reuters...\n",
              "3      1  Iraq Halts Oil Exports from Main Southern Pipe...\n",
              "4      1  Oil prices soar to all-time record, posing new...\n",
              "5      1  Stocks End Up, But Near Year Lows (Reuters) Re...\n",
              "6      1  Money Funds Fell in Latest Week (AP) AP - Asse...\n",
              "7      1  Fed minutes show dissent over inflation (USATO...\n",
              "8      1  Safety Net (Forbes.com) Forbes.com - After ear...\n",
              "9      1  Wall St. Bears Claw Back Into the Black  NEW Y..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-206740ec-4996-40e9-9be9-48a7b4d52a5c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-206740ec-4996-40e9-9be9-48a7b4d52a5c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-206740ec-4996-40e9-9be9-48a7b4d52a5c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-206740ec-4996-40e9-9be9-48a7b4d52a5c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_df",
              "summary": "{\n  \"name\": \"processed_df\",\n  \"rows\": 90000,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90000,\n        \"samples\": [\n          \"Illinois Governor Seeks Restrictions on Video Game Sales Description: Illinois Gov. Rod Blagojevich is backing state legislation that would ban sales or rentals of video games with graphic sexual or violent content to children under 18.\",\n          \"USTA nets server savings IBM, which manages computer operations for the United States Tennis Association, has consolidated Internet scoring, staging and Web publishing applications onto a single eServer i5 system to support the U.S. Open's processing requirements.\",\n          \"Yahoo, Adobe team on Net services com October 25, 2004, 1:49 PM PT. This priority retains its ranking at number five as more and more companies deploy Web Services to share business logic, data and processes with each other and with clients.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to process text, we need to identify features that our model can use to classify texts.\n",
        "\n",
        "We use four hand-crafted features:\n",
        "\n",
        "1.  natural logarithm of text length;\n",
        "2.  number of sports words in the text;\n",
        "3.  number of business words in the text;\n",
        "4.  number of science words in the text.\n",
        "\n",
        "Next, we download a list of words for each target label to compute our features. We get the lists of words from [Enchanted Learning - Wordlist](https://www.enchantedlearning.com/wordlist/)."
      ],
      "metadata": {
        "id": "n3fOaD825c8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XWzN3nBPcWp50f_DjC2rpS7G2PvzjvmB' -O business.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1u3KQGgkFTN8s4fTGzJJOsmR4MGVlusp1' -O science.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1g_gYaij_xn1HwGebruK3JJRZy30P7E49' -O sports.txt\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "tAghO6DnxUdt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes the features of the given text and returns a list of features in the format:\n",
        "# [ln text_len, # sports words, # business words, # science words]\n",
        "def get_features_text(text: str):\n",
        "  \"\"\"\n",
        "    Calculated text features for the given text.\n",
        "\n",
        "    Args:\n",
        "        text (string): string containing the text.\n",
        "\n",
        "    Returns:\n",
        "        List with dimension 4: list containing the features extracted from the text in the order\n",
        "                                [ln text_len, # sports words, # business words, # science words].\n",
        "    \"\"\"\n",
        "  import math\n",
        "  # divide the text into words\n",
        "  words = text.split()\n",
        "\n",
        "  # initialise the features list adding the ln of the text length\n",
        "  features = [math.log(len(words))]\n",
        "\n",
        "  # for each target label we have in the dataset\n",
        "  for label in labels:\n",
        "    # open the list of words of the current label\n",
        "    with open(f'{label}.txt', 'r') as label_file:\n",
        "      # get all the words associated to the current label\n",
        "      # the file contains 1 word for each line, so we split the whole text with \\n\n",
        "      label_words = label_file.read().split('\\n')\n",
        "    label_count = 0\n",
        "    # for each word in the text,\n",
        "    for word in words:\n",
        "      # check if the word is present in the list of words associated to the current label\n",
        "      if word in label_words:\n",
        "        # if it's present add 1 to the current count\n",
        "        label_count += 1\n",
        "    # append the final count to the features list\n",
        "    features.append(label_count)\n",
        "  # return the features list [ln text_len, # sports words, # business words, # science words]\n",
        "  return features"
      ],
      "metadata": {
        "id": "2rzItBYIvLrT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new column containing the features calculated as described above\n",
        "processed_df['text_features'] = [get_features_text(text) for text in processed_df['text']]\n",
        "processed_df.head(10)"
      ],
      "metadata": {
        "id": "0UqpNoEFxZ-E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "b06d2529-c3a8-455a-cbe7-845c1a0f7d66"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text  \\\n",
              "0      1  Wall St. Bears Claw Back Into the Black (Reute...   \n",
              "1      1  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
              "2      1  Oil and Economy Cloud Stocks' Outlook (Reuters...   \n",
              "3      1  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
              "4      1  Oil prices soar to all-time record, posing new...   \n",
              "5      1  Stocks End Up, But Near Year Lows (Reuters) Re...   \n",
              "6      1  Money Funds Fell in Latest Week (AP) AP - Asse...   \n",
              "7      1  Fed minutes show dissent over inflation (USATO...   \n",
              "8      1  Safety Net (Forbes.com) Forbes.com - After ear...   \n",
              "9      1  Wall St. Bears Claw Back Into the Black  NEW Y...   \n",
              "\n",
              "                   text_features  \n",
              "0   [3.044522437723423, 0, 0, 0]  \n",
              "1    [3.58351893845611, 1, 1, 0]  \n",
              "2    [3.58351893845611, 0, 3, 0]  \n",
              "3    [3.58351893845611, 0, 0, 0]  \n",
              "4  [3.6109179126442243, 0, 1, 0]  \n",
              "5  [3.6375861597263857, 0, 0, 1]  \n",
              "6  [3.5553480614894135, 0, 3, 0]  \n",
              "7  [3.6635616461296463, 0, 4, 0]  \n",
              "8   [4.219507705176107, 1, 8, 0]  \n",
              "9  [3.1354942159291497, 0, 0, 0]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cf17e76c-a4bf-4477-9f52-a43de6cf772a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reute...</td>\n",
              "      <td>[3.044522437723423, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>[3.58351893845611, 1, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters...</td>\n",
              "      <td>[3.58351893845611, 0, 3, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>[3.58351893845611, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>[3.6109179126442243, 0, 1, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>Stocks End Up, But Near Year Lows (Reuters) Re...</td>\n",
              "      <td>[3.6375861597263857, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>Money Funds Fell in Latest Week (AP) AP - Asse...</td>\n",
              "      <td>[3.5553480614894135, 0, 3, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed minutes show dissent over inflation (USATO...</td>\n",
              "      <td>[3.6635616461296463, 0, 4, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>Safety Net (Forbes.com) Forbes.com - After ear...</td>\n",
              "      <td>[4.219507705176107, 1, 8, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black  NEW Y...</td>\n",
              "      <td>[3.1354942159291497, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf17e76c-a4bf-4477-9f52-a43de6cf772a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cf17e76c-a4bf-4477-9f52-a43de6cf772a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cf17e76c-a4bf-4477-9f52-a43de6cf772a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "processed_df",
              "summary": "{\n  \"name\": \"processed_df\",\n  \"rows\": 90000,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 90000,\n        \"samples\": [\n          \"Illinois Governor Seeks Restrictions on Video Game Sales Description: Illinois Gov. Rod Blagojevich is backing state legislation that would ban sales or rentals of video games with graphic sexual or violent content to children under 18.\",\n          \"USTA nets server savings IBM, which manages computer operations for the United States Tennis Association, has consolidated Internet scoring, staging and Web publishing applications onto a single eServer i5 system to support the U.S. Open's processing requirements.\",\n          \"Yahoo, Adobe team on Net services com October 25, 2004, 1:49 PM PT. This priority retains its ranking at number five as more and more companies deploy Web Services to share business logic, data and processes with each other and with clients.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract X (text_featues) and y (label) from the dataset\n",
        "X = np.array(processed_df['text_features'].to_list())\n",
        "y_train = processed_df['label'].to_list()"
      ],
      "metadata": {
        "id": "yy6oXD2uiGnJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset shapes\n",
        "print(f\"Dataset inputs or features shape (num_samples, num_features): {X.shape}\") # -> (90000, 4)\n",
        "assert X.shape == (90000, 4)\n",
        "print(f\"Dataset outputs or labels shape (num_samples, ): {len(y_train)}\") # -> 90000\n",
        "assert len(y_train) == 90000"
      ],
      "metadata": {
        "id": "N8qUB6GxAVx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd71c850-11a2-49f8-f03d-e1090554fc73"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset inputs or features shape (num_samples, num_features): (90000, 4)\n",
            "Dataset outputs or labels shape (num_samples, ): 90000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Data Standardisation\n",
        "\n",
        "We need to standardise our data for the same reasons as discussed for simple logistic regression above, but since our training and test set are in different files we will package the data standardisation in a function, so we can reuse it: first to standardise the training set, and then for the test set during evaluation."
      ],
      "metadata": {
        "id": "phoIJhv7ak8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We commented all print statements for clean code display -- uncomment as needed\n",
        "def standardise_features(X: np.ndarray):\n",
        "    \"\"\"\n",
        "      Standardise the features contained in the given dataset.\n",
        "\n",
        "      Args:\n",
        "          X (array): array with dimension (num_samples, num_features).\n",
        "\n",
        "      Returns:\n",
        "          array with dimension (num_samples, num_features): dataset with standardised features.\n",
        "    \"\"\"\n",
        "    # Check means and standard deviations for each features\n",
        "    means = X.mean(axis=0)\n",
        "    stds = X.std(axis=0)\n",
        "\n",
        "    #print(\"Means of features:\", means)\n",
        "    #print(\"Standard deviations of features:\", stds)\n",
        "\n",
        "    # If not standardised, apply Standard Scaling (Z-score standardisation)\n",
        "    if any(stds != 1) or any(means != 0):\n",
        "        scaler = StandardScaler()\n",
        "        standardised_X = scaler.fit_transform(X)\n",
        "        #after_means = X.mean(axis=0)\n",
        "        #after_stds = X.std(axis=0)\n",
        "        #print(\"\\nMeans of features after standardisation:\", after_means)\n",
        "        #print(\"Standard deviations of features after standardisation:\", after_stds)\n",
        "    else:\n",
        "        print(\"Dataset is already standardised.\")\n",
        "    return standardised_X"
      ],
      "metadata": {
        "id": "KZ0C13sAas7w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_standardised = standardise_features(X) #standardise train set"
      ],
      "metadata": {
        "id": "hWUhR-JreJCP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did before, we need to initialise the Bias term ($b$) and concatenate it to the dataset."
      ],
      "metadata": {
        "id": "5dn_srAwHWus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add bias term to X\n",
        "X_train = np.c_[np.ones(X_standardised.shape[0]), X]"
      ],
      "metadata": {
        "id": "0PL6xm-6mi6T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check training sets shapes\n",
        "print(f\"Training set shape (num_samples x (num_features+bias)): {X_train.shape}\") # -> (90000, 5)\n",
        "assert X_train.shape == (90000, 5)"
      ],
      "metadata": {
        "id": "m7cO91Kqhuks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2f32fef-c1c2-4926-853f-ad00733e5b33"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape (num_samples x (num_features+bias)): (90000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Logistic Function: Softmax Function definition\n",
        "\n",
        "Following the definition of Softmax function\n",
        "\n",
        "**$\\text{softmax}(Z_i) = \\frac{\\text{exp}(Z_i)}{\\sum_{j=1}^{K} \\text{exp}(Z_j)}$**\n",
        "\n",
        " implement a function that calculates the Softmax for a given Z.\n",
        "\n",
        " The sum of the exponentials ($\\sum_{j=1}^{K} \\text{exp}(Z_j)$) may lead to very large numbers, this could cause instability in calculating the gradients. To avoid this, we reduce each value $Z_i$ by subtracting from the largest value in the input ([Softmax Function Reference](https://en.wikipedia.org/wiki/Softmax_function)):\n",
        "\n",
        "\n",
        "\n",
        " **$\\text{softmax}(Z_i) = \\frac{\\text{exp}(Z_i - \\text{max}(Z))}{\\sum_{j=1}^{K} \\text{exp}(Z_j - \\text{max}(Z))}$**\n",
        "\n",
        " Hint: use numpy to calculate the exponential, the max and the sum.\n",
        "\n",
        " **Note:** As we will take the max and the sum along each row (that means we perform these operations on columns), this will result in a matrix with only one dimension instead of two, and becuase there are other operators (e.g. exponential) that don't change the number of dimensions, we need to keep the dimension when applying the max and the sum operations (Search for parameters in max and sum numpy reference that can keep the dimensions)."
      ],
      "metadata": {
        "id": "SukxZloYHjlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z: np.ndarray):\n",
        "  \"\"\"\n",
        "    Softmax function that gives us the predictions of the given samples using our model.\n",
        "\n",
        "    Args:\n",
        "        Z (array): array with dimension (num_train_samples, num_class), each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "    Returns:\n",
        "        array with dimension (num_train_samples, num_class): array containing a prediction made by the model for each sample.\n",
        "  \"\"\"\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  exp_Z = (np.exp(Z - np.max(Z)))/np.sum(np.exp(Z) - np.max(Z))\n",
        "  return exp_Z\n",
        "  ## END OF YOUR CODE ##"
      ],
      "metadata": {
        "id": "2YKRlNfNHkDe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Training Phase"
      ],
      "metadata": {
        "id": "hsooAXoi9pJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 Initialise Weights \\( $w_1, w_2, \\ldots, w_n$ \\)\n",
        "\n",
        "We need to initialise 1 weight for each class in the dataset for each feature composing a sample in our dataset.\n",
        "\n",
        "First, you need to identify the number of classes, then you can inizialise the correct number of weights.\n",
        "\n",
        "You can initialise all the weights to 0."
      ],
      "metadata": {
        "id": "7qw7PfewH1lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights\n",
        "\n",
        "# Before, we had a single vector of weights with dimension (6,)\n",
        "# Now we need 1 weight for each feature+bias and for each class in the dataset\n",
        "# obtaining a matrix with dimensions (num_features+bias, num_classes).\n",
        "# The initialisation is similar to before, but we need to identify the number of classes\n",
        "# to initialise a matrix with the correct dimensions.\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "# num_classes = 3\n",
        "num_classes = len(np.unique(y_train))\n",
        "weights = np.zeros((X_train.shape[1], num_classes))\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "# Check weights shape\n",
        "print(f\"Weights Shape (num_features+bias, num_classes): {weights.shape}\") # -> (5, 3)\n",
        "assert weights.shape == (5, 3)"
      ],
      "metadata": {
        "id": "1_kFqF_nH2RG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9477e598-92b1-4fa5-a1d7-eb10665635e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Shape (num_features+bias, num_classes): (5, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.2 Hyperparameters\n",
        "\n"
      ],
      "metadata": {
        "id": "F8A2ahbpH6IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.001\n",
        "num_iterations = 5000"
      ],
      "metadata": {
        "id": "3dnBeYalH6Ys"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.3 Training Loop\n",
        "\n",
        "In each training iteration, we need to perform different steps:\n",
        "\n",
        "1.   Calculate Z\n",
        "2.   Use Z to get the predictions of the samples using our model;\n",
        "3.   Compute the error (loss function) between the predictions and the real labels in the dataset (*y_train*);\n",
        "4.   Calculate the gradient based on the obtained error;\n",
        "5.   Update the weights.\n",
        "\n",
        "You may use numpy functions [np.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) and [np.eye](https://numpy.org/devdocs/reference/generated/numpy.eye.html)."
      ],
      "metadata": {
        "id": "pqQPtnNOIIp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "# You can comment the print functions for a clear output after you complete coding all the training steps.\n",
        "losses = []\n",
        "for i in tqdm(range(num_iterations), total=num_iterations, desc=\"Training\"):\n",
        "\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  # 1. Calculate Z\n",
        "  # X_train: (num_train_samples, num_features+bias) x weights: (num_features+bias, num_class) -> Z: (num_train_samples, num_class)\n",
        "  Z = np.dot(X_train, weights)\n",
        "  if i==0:\n",
        "    print('Z Shape (num_train_samples, num_class):', Z.shape) # -> (90000, 3)\n",
        "    assert Z.shape == (90000, 3)\n",
        "\n",
        "  # 2. Get predictions\n",
        "  # Z: (num_train_samples, num_class) -> predictions: (num_train_samples, num_class)\n",
        "  predictions = softmax(Z)\n",
        "  if i==0:\n",
        "    print('Predictions Shape (num_train_samples, num_class):', predictions.shape) # -> (90000, 3)\n",
        "    assert predictions.shape == (90000, 3)\n",
        "\n",
        "  # 3. Compute error between prediction and real label\n",
        "  '''\n",
        "  The prediction for each data item is a vector of probabilites summing to 1.\n",
        "  For example, if we have a prediction for a data point like this: [0.2, 0.5, 0.3],\n",
        "  we can read this as there is 0.2 probability that the data item belongs to the first class,\n",
        "  0.5 to the second class, and 0.3 to the third class. Our gold label is just the index\n",
        "  of the class, taking the values 0, 1 or 2. Let's assume our gold label is 2.\n",
        "  To compare our predictions to the label, we need to represent the label in a comparable format.\n",
        "  To achieve this, we can use one-hot encoding for the labels (setting 1 in the desired index and 0 elsewhere),\n",
        "  giving the following for a label of 2: [0, 0, 1].\n",
        "  This we can compare to the prediction, finding that we are off by 0.7 from the perfect true class probability (1).\n",
        "\n",
        "  Hint: to get the one-hot encoding of the labels (y_train in our case), think about using np.eye() function\n",
        "  '''\n",
        "  # predictions: (num_train_samples, num_class) y_train: (num_train_samples, ) -> errors: (num_train_samples, num_class)\n",
        "  y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "  errors = predictions - y_train_one_hot\n",
        "  if i==0:\n",
        "    print('Error Shape (num_train_samples, ):', errors.shape)  # -> (90000, 3)\n",
        "    assert errors.shape == (90000, 3)\n",
        "\n",
        "  # 4. Calculate gradient\n",
        "  #  We transpose X_train to align the dimensions correctly with the error dimension for matrix multiplication.\n",
        "  # X_train.T: (num_features+bias, num_train_samples) x error: (num_train_samples, num_class) -> gradient or change in weights vector: (num_features+bias, num_class)\n",
        "  gradients = np.dot(X_train.T, errors)\n",
        "  if i==0:\n",
        "    print('Gradient Shape (num_features+bias, num_class):', gradients.shape)  # -> (5, 3)\n",
        "    assert gradients.shape == (5, 3)\n",
        "\n",
        "  # 5. Update weights\n",
        "  # gradient: (num_features+bias, num_class) -> weights: (num_features+bias, num_class)\n",
        "  weights -= learning_rate * gradients\n",
        "  if i==0:\n",
        "    print('Weights Shape (num_features+bias, num_class):', weights.shape) # -> (5, 3)\n",
        "    assert weights.shape == (5, 3)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  loss = -np.sum(y_train_one_hot * np.log(predictions+ 1e-15)) / len(y_train) # Add small epsilon to avoid log(0)\n",
        "  losses.append(loss)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mk3By3BSIFEO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "0c2550c4-a2c4-4d1f-ffcf-90331a1b4e52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/5000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z Shape (num_train_samples, num_class): (90000, 3)\n",
            "Predictions Shape (num_train_samples, num_class): (90000, 3)\n",
            "Error Shape (num_train_samples, ): (90000, 3)\n",
            "Gradient Shape (num_features+bias, num_class): (5, 3)\n",
            "Weights Shape (num_features+bias, num_class): (5, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4001610020.py:12: RuntimeWarning: overflow encountered in exp\n",
            "  exp_Z = (np.exp(Z - np.max(Z)))/np.sum(np.exp(Z) - np.max(Z))\n",
            "Training: 100%|██████████| 5000/5000 [04:09<00:00, 20.04it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the loss function plotted is not decreasing as we observed in Section 1 because the features we extracted from the texts are not representative enough to allow the model to learn proper decision boundaries to select the correct label."
      ],
      "metadata": {
        "id": "Rm3U5KtGsV-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Evaluate the trained model\n",
        "\n",
        "As we did in Section 1.5, we want to evaluate the trained model to check the performance.\n",
        "\n",
        "1. We preprocess the test set as we did for the train set.\n",
        "2. We use the final weights obtained from training and we compute the predictions of our test set.\n",
        "3. We compare the predictions against the target labels to calculate the accuracy of our model."
      ],
      "metadata": {
        "id": "bUOyqWzOIKcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepocess_data(filename):\n",
        "  \"\"\"\n",
        "    Preprocess the given dataset to set it in the correct format, i.e.\n",
        "    containing 2 columns (label, text_features) with standardised text features and adding bias to the features.\n",
        "    This function collects all the steps described in details in Sections 2.1 and 2.2\n",
        "\n",
        "    Args:\n",
        "        filename (string): name of the file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        X: bias+features extracted from the samples in the dataset.\n",
        "        y: gold labels of the dataset.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename, header=None)\n",
        "  df = df[df[0] != 1]\n",
        "  df['label'] = df[0]-2\n",
        "  df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "  df = df.drop(columns=[0,1,2])\n",
        "\n",
        "  df['text_features'] = [get_features_text(text) for text in df['text']]\n",
        "\n",
        "  X = np.array(df['text_features'].to_list())\n",
        "  X_standardised = standardise_features(X) #standardise test set\n",
        "  X = np.c_[np.ones(X_standardised.shape[0]), X_standardised]\n",
        "\n",
        "  y = df['label'].to_list()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X_test, y_test = prepocess_data('agnews_test.csv')\n",
        "\n",
        "print(f\"Test set shape (num_test_samples , (num_features+bias)): {X_test.shape}\") # -> (5700, 5)\n",
        "assert X_test.shape == (5700, 5)"
      ],
      "metadata": {
        "id": "nI7nhG41zg66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30101277-4f0a-4611-e9f1-09b318b2d788"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set shape (num_test_samples , (num_features+bias)): (5700, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test set using trained weights\n",
        "# X_test: (num_test_samples, num_features) x weights: (num_features, num_class) -> Z: (num_samples, num_class)\n",
        "z = np.dot(X_test, weights)\n",
        "test_predictions = softmax(z)\n",
        "\n",
        "# select the class with the highest probability\n",
        "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "89Egw08cIN1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19193618-3704-4b93-ccfb-e586b1a76c73"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4001610020.py:12: RuntimeWarning: overflow encountered in exp\n",
            "  exp_Z = (np.exp(Z - np.max(Z)))/np.sum(np.exp(Z) - np.max(Z))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Identify new text features (__Week 1 Submission Exercise__)\n",
        "\n",
        "Using the code from Section 2 on Multinomial Logistic Regression as a starting point, create new text features for the same multi-class task and try to obtain better performance.\n",
        "\n",
        "You can modify the features in the code provided in Section 2.1, in function `get_features_text`.\n",
        "\n",
        "Copy all and only the required code to a new notebook, and submit it for Week 1. Make sure you print both original performance (with the features used in Section 2) and performance for your new features to screen.\n",
        "\n",
        "Consult the module handbook for instructions how to share the weekly submission exercises with us."
      ],
      "metadata": {
        "id": "YDj_byJ4EMQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "- Speech and Language Processing (3rd ed. draft) Dan Jurafsky and James H. Martin, [Chapter 4](https://web.stanford.edu/~jurafsky/slp3/4.pdf)\n",
        "- [Enchanted Learning - Wordlist](https://www.enchantedlearning.com/wordlist/)\n",
        "- [AG News dataset](https://huggingface.co/datasets/wangrongsheng/ag_news)"
      ],
      "metadata": {
        "id": "LnF13fbPIark"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Creating new features\n",
        "In order to improve performance we need to hand design more features that can help in classifying whether the text is part of one class more than another."
      ],
      "metadata": {
        "id": "LHFAlkors5Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1QfAUt0u4wLZVy2Ta1G90jOLNaqzAw2eW' -O agnews_test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UsOBTnfch-Su4kqmkzXcIizwJt6NWtXZ' -O agnews_train.csv\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XWzN3nBPcWp50f_DjC2rpS7G2PvzjvmB' -O business.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1u3KQGgkFTN8s4fTGzJJOsmR4MGVlusp1' -O science.txt\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1g_gYaij_xn1HwGebruK3JJRZy30P7E49' -O sports.txt\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "yl5F8Lt5XiJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# labels contained in the dataset\n",
        "labels = ['sports', 'business', 'science']\n",
        "\n",
        "# dataset is saved in a CSV file with no header and each column in separated by comma\n",
        "# the file has the following structure:\n",
        "# gold_label , title , body\n",
        "df = pd.read_csv('agnews_train.csv', header=None)\n",
        "\n",
        "# in the dataset gold labels are given as: 1 (World), 2 (Sports), 3 (Business), 4 (Science/Technology)\n",
        "# we discard all the rows with gold label 1 (World) and we keep all the other rows\n",
        "df = df[df[0] != 1]\n",
        "\n",
        "# we create a 'label' column, subtract 2 from each gold label so we obtain a direct mapping with our list of labels:\n",
        "# 0 -> sports ; 1 -> business ; 2 -> science\n",
        "df['label'] = df[0]-2\n",
        "\n",
        "# we concatenate title and body to obtain a unified text\n",
        "df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "# remove the original 3 columns to obtain our final processed dataset, containing 2 columns: label and text\n",
        "# the original 3 columns are the unmatched labels, the titles and the bodies\n",
        "processed_df = df.drop(columns=[0,1,2])\n",
        "processed_df.head(10)\n",
        "\n",
        "# computes the features of the given text and returns a list of features in the format:\n",
        "# [ln text_len, # sports words, # business words, # science words]\n",
        "def get_features_text(text: str):\n",
        "  \"\"\"\n",
        "    Calculated text features for the given text.\n",
        "\n",
        "    Args:\n",
        "        text (string): string containing the text.\n",
        "\n",
        "    Returns:\n",
        "        List with dimension 4: list containing the features extracted from the text in the order\n",
        "                                [ln text_len, # sports words, # business words, # science words].\n",
        "    \"\"\"\n",
        "  import math\n",
        "  # divide the text into words\n",
        "  words = text.split()\n",
        "\n",
        "  # initialise the features list adding the ln of the text length\n",
        "  features = [math.log(len(words))]\n",
        "\n",
        "  # for each target label we have in the dataset\n",
        "  for label in labels:\n",
        "    # open the list of words of the current label\n",
        "    with open(f'{label}.txt', 'r') as label_file:\n",
        "      # get all the words associated to the current label\n",
        "      # the file contains 1 word for each line, so we split the whole text with \\n\n",
        "      label_words = label_file.read().split('\\n')\n",
        "    label_count = 0\n",
        "    # for each word in the text,\n",
        "    for word in words:\n",
        "      word = word.strip().lower()\n",
        "      # check if the word is present in the list of words associated to the current label\n",
        "      if word in label_words:\n",
        "        # if it's present add 1 to the current count\n",
        "        label_count += 1\n",
        "    # append the final count to the features list\n",
        "    features.append(label_count)\n",
        "  # return the features list [ln text_len, # sports words, # business words, # science words]\n",
        "  return features\n",
        "\n",
        "# create a new column containing the features calculated as described above\n",
        "processed_df['text_features'] = [get_features_text(text) for text in processed_df['text']]\n",
        "processed_df.head(10)\n",
        "\n",
        "# extract X (text_featues) and y (label) from the dataset\n",
        "X = np.array(processed_df['text_features'].to_list())\n",
        "y_train = processed_df['label'].to_list()\n",
        "\n",
        "# Check dataset shapes\n",
        "print(f\"Dataset inputs or features shape (num_samples, num_features): {X.shape}\") # -> (90000, 4)\n",
        "assert X.shape == (90000, 4)\n",
        "print(f\"Dataset outputs or labels shape (num_samples, ): {len(y_train)}\") # -> 90000\n",
        "assert len(y_train) == 90000\n",
        "\n",
        "#We commented all print statements for clean code display -- uncomment as needed\n",
        "def standardise_features(X: np.ndarray):\n",
        "    \"\"\"\n",
        "      Standardise the features contained in the given dataset.\n",
        "\n",
        "      Args:\n",
        "          X (array): array with dimension (num_samples, num_features).\n",
        "\n",
        "      Returns:\n",
        "          array with dimension (num_samples, num_features): dataset with standardised features.\n",
        "    \"\"\"\n",
        "    # Check means and standard deviations for each features\n",
        "    means = X.mean(axis=0)\n",
        "    stds = X.std(axis=0)\n",
        "\n",
        "    #print(\"Means of features:\", means)\n",
        "    #print(\"Standard deviations of features:\", stds)\n",
        "\n",
        "    # If not standardised, apply Standard Scaling (Z-score standardisation)\n",
        "    if any(stds != 1) or any(means != 0):\n",
        "        scaler = StandardScaler()\n",
        "        standardised_X = scaler.fit_transform(X)\n",
        "        #after_means = X.mean(axis=0)\n",
        "        #after_stds = X.std(axis=0)\n",
        "        #print(\"\\nMeans of features after standardisation:\", after_means)\n",
        "        #print(\"Standard deviations of features after standardisation:\", after_stds)\n",
        "    else:\n",
        "        print(\"Dataset is already standardised.\")\n",
        "    return standardised_X\n",
        "\n",
        "X_standardised = standardise_features(X) #standardise train set\n",
        "\n",
        "# Add bias term to X\n",
        "X_train = np.c_[np.ones(X_standardised.shape[0]), X]\n",
        "\n",
        "# Check training sets shapes\n",
        "print(f\"Training set shape (num_samples x (num_features+bias)): {X_train.shape}\") # -> (90000, 5)\n",
        "assert X_train.shape == (90000, 5)\n",
        "\n",
        "def softmax(Z: np.ndarray):\n",
        "  \"\"\"\n",
        "    Softmax function that gives us the predictions of the given samples using our model.\n",
        "\n",
        "    Args:\n",
        "        Z (array): array with dimension (num_train_samples, num_class), each row contains z = b + w_1 x_1 + w_2 x_2 + ... + w_n x_n\n",
        "\n",
        "    Returns:\n",
        "        array with dimension (num_train_samples, num_class): array containing a prediction made by the model for each sample.\n",
        "  \"\"\"\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "  return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  # Initialize weights\n",
        "\n",
        "# Before, we had a single vector of weights with dimension (6,)\n",
        "# Now we need 1 weight for each feature+bias and for each class in the dataset\n",
        "# obtaining a matrix with dimensions (num_features+bias, num_classes).\n",
        "# The initialisation is similar to before, but we need to identify the number of classes\n",
        "# to initialise a matrix with the correct dimensions.\n",
        "\n",
        "## INSERT YOUR CODE HERE ##\n",
        "num_classes = len(np.unique(y_train))\n",
        "weights = np.zeros((X_train.shape[1], num_classes))\n",
        "## END OF YOUR CODE ##\n",
        "\n",
        "# Check weights shape\n",
        "print(f\"Weights Shape (num_features+bias, num_classes): {weights.shape}\") # -> (5, 3)\n",
        "assert weights.shape == (5, 3)\n",
        "\n",
        "# Learning rate and number of iterations\n",
        "learning_rate = 0.001\n",
        "num_iterations = 5000\n",
        "\n",
        "# ------------------------------------\n",
        "'''new code'''\n",
        "# ------------------------------------\n",
        "# Training the model\n",
        "# You can comment the print functions for a clear output after you complete coding all the training steps.\n",
        "losses = []\n",
        "for i in tqdm(range(num_iterations), total=num_iterations, desc=\"Training\"):\n",
        "\n",
        "  ## INSERT YOUR CODE HERE ##\n",
        "  # 1. Calculate Z\n",
        "  # X_train: (num_train_samples, num_features+bias) x weights: (num_features+bias, num_class) -> Z: (num_train_samples, num_class)\n",
        "  Z = np.dot(X_train, weights)\n",
        "  if i==0:\n",
        "    print('Z Shape (num_train_samples, num_class):', Z.shape) # -> (90000, 3)\n",
        "    assert Z.shape == (90000, 3)\n",
        "\n",
        "  # 2. Get predictions\n",
        "  # Z: (num_train_samples, num_class) -> predictions: (num_train_samples, num_class)\n",
        "  predictions = softmax(Z)\n",
        "  if i==0:\n",
        "    print('Predictions Shape (num_train_samples, num_class):', predictions.shape) # -> (90000, 3)\n",
        "    assert predictions.shape == (90000, 3)\n",
        "\n",
        "  # 3. Compute error between prediction and real label\n",
        "  '''\n",
        "  The prediction for each data item is a vector of probabilites summing to 1.\n",
        "  For example, if we have a prediction for a data point like this: [0.2, 0.5, 0.3],\n",
        "  we can read this as there is 0.2 probability that the data item belongs to the first class,\n",
        "  0.5 to the second class, and 0.3 to the third class. Our gold label is just the index\n",
        "  of the class, taking the values 0, 1 or 2. Let's assume our gold label is 2.\n",
        "  To compare our predictions to the label, we need to represent the label in a comparable format.\n",
        "  To achieve this, we can use one-hot encoding for the labels (setting 1 in the desired index and 0 elsewhere),\n",
        "  giving the following for a label of 2: [0, 0, 1].\n",
        "  This we can compare to the prediction, finding that we are off by 0.7 from the perfect true class probability (1).\n",
        "\n",
        "  Hint: to get the one-hot encoding of the labels (y_train in our case), think about using np.eye() function\n",
        "  '''\n",
        "  # predictions: (num_train_samples, num_class) y_train: (num_train_samples, ) -> errors: (num_train_samples, num_class)\n",
        "  y_train_one_hot = np.eye(num_classes)[y_train]\n",
        "  errors = predictions - y_train_one_hot\n",
        "  if i==0:\n",
        "    print('Error Shape (num_train_samples, ):', errors.shape)  # -> (90000, 3)\n",
        "    assert errors.shape == (90000, 3)\n",
        "\n",
        "  # 4. Calculate gradient\n",
        "  #  We transpose X_train to align the dimensions correctly with the error dimension for matrix multiplication.\n",
        "  # X_train.T: (num_features+bias, num_train_samples) x error: (num_train_samples, num_class) -> gradient or change in weights vector: (num_features+bias, num_class)\n",
        "  gradients = np.dot(X_train.T, errors)\n",
        "  if i==0:\n",
        "    print('Gradient Shape (num_features+bias, num_class):', gradients.shape)  # -> (5, 3)\n",
        "    assert gradients.shape == (5, 3)\n",
        "\n",
        "  # 5. Update weights\n",
        "  # gradient: (num_features+bias, num_class) -> weights: (num_features+bias, num_class)\n",
        "  weights -= learning_rate * gradients\n",
        "  if i==0:\n",
        "    print('Weights Shape (num_features+bias, num_class):', weights.shape) # -> (5, 3)\n",
        "    assert weights.shape == (5, 3)\n",
        "  ## END OF YOUR CODE ##\n",
        "\n",
        "  loss = -np.sum(y_train_one_hot * np.log(predictions+ 1e-15)) / len(y_train) # Add small epsilon to avoid log(0)\n",
        "  losses.append(loss)\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "\n",
        "def prepocess_data(filename):\n",
        "  \"\"\"\n",
        "    Preprocess the given dataset to set it in the correct format, i.e.\n",
        "    containing 2 columns (label, text_features) with standardised text features and adding bias to the features.\n",
        "    This function collects all the steps described in details in Sections 2.1 and 2.2\n",
        "\n",
        "    Args:\n",
        "        filename (string): name of the file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        X: bias+features extracted from the samples in the dataset.\n",
        "        y: gold labels of the dataset.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv(filename, header=None)\n",
        "  df = df[df[0] != 1]\n",
        "  df['label'] = df[0]-2\n",
        "  df['text'] = [\" \".join((title, body)) for title, body in zip(df[1], df[2])]\n",
        "  df = df.drop(columns=[0,1,2])\n",
        "\n",
        "  df['text_features'] = [get_features_text(text) for text in df['text']]\n",
        "\n",
        "  X = np.array(df['text_features'].to_list())\n",
        "  X_standardised = standardise_features(X) #standardise test set\n",
        "  X = np.c_[np.ones(X_standardised.shape[0]), X_standardised]\n",
        "\n",
        "  y = df['label'].to_list()\n",
        "\n",
        "  return X, y\n",
        "\n",
        "X_test, y_test = prepocess_data('agnews_test.csv')\n",
        "\n",
        "print(f\"Test set shape (num_test_samples , (num_features+bias)): {X_test.shape}\") # -> (5700, 5)\n",
        "assert X_test.shape == (5700, 5)\n",
        "\n",
        "# Making predictions on the test set using trained weights\n",
        "# X_test: (num_test_samples, num_features) x weights: (num_features, num_class) -> Z: (num_samples, num_class)\n",
        "z = np.dot(X_test, weights)\n",
        "test_predictions = softmax(z)\n",
        "\n",
        "# select the class with the highest probability\n",
        "multiclass_predictions = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Evaluating the model\n",
        "accuracy = accuracy_score(y_test, multiclass_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "NfdPy_0cW36E",
        "outputId": "8729d9a4-c547-437e-d7a4-783e85e9f6bc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset inputs or features shape (num_samples, num_features): (90000, 4)\n",
            "Dataset outputs or labels shape (num_samples, ): 90000\n",
            "Training set shape (num_samples x (num_features+bias)): (90000, 5)\n",
            "Weights Shape (num_features+bias, num_classes): (5, 3)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 3/5000 [00:00<03:38, 22.90it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z Shape (num_train_samples, num_class): (90000, 3)\n",
            "Predictions Shape (num_train_samples, num_class): (90000, 3)\n",
            "Error Shape (num_train_samples, ): (90000, 3)\n",
            "Gradient Shape (num_features+bias, num_class): (5, 3)\n",
            "Weights Shape (num_features+bias, num_class): (5, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 5000/5000 [04:19<00:00, 19.27it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set shape (num_test_samples , (num_features+bias)): (5700, 5)\n",
            "Accuracy: 0.5178947368421053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "current accuracy = 0.48473"
      ],
      "metadata": {
        "id": "mOKFTf3peYwE"
      }
    }
  ]
}